{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a9a3d9",
   "metadata": {},
   "source": [
    "2024-05-31: final check; this version is used for the arxiv 1.0 version of paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cedb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "clemrun_base = '/Users/das/work/svn/Gits/a_Projects/101_clembench/clembench-runs'\n",
    "\n",
    "v1 = pd.read_csv(clemrun_base + '/v1.0/results.csv')\n",
    "v1 = v1.rename(columns={v1.columns[0]: 'models'})\n",
    "v1 = v1.set_index('models')\n",
    "\n",
    "colname_score = v1.columns[0]\n",
    "colname_played = v1.columns[1]\n",
    "colname_quality = v1.columns[2]\n",
    "\n",
    "v2 = pd.read_csv(clemrun_base + '/v1.6/results.csv')\n",
    "v2 = v2.rename(columns={v2.columns[0]: 'models'})\n",
    "v2 = v2.set_index('models')\n",
    "\n",
    "both = set(v1.index).intersection(set(v2.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c58c41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/1669827204.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  v2.index = v2.index.str.replace(r'(\\w*)-t0.0.*', r'\\1').str.lower()\n"
     ]
    }
   ],
   "source": [
    "v2.index = v2.index.str.replace(r'(\\w*)-t0.0.*', r'\\1').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce000d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/622844938.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  v1.index = v1.index.str.replace(r'(\\w*)-t0.0.*', r'\\1').str.lower()\n"
     ]
    }
   ],
   "source": [
    "v1.index = v1.index.str.replace(r'(\\w*)-t0.0.*', r'\\1').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc9ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = pd.read_csv(clemrun_base + '/v0.9/results.csv')\n",
    "v0 = v0.rename(columns={v0.columns[0]: 'models'})\n",
    "v0 = v0.set_index('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53496675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claude-v1.3-t0.0--claude-v1.3-t0.0',\n",
       "       'falcon-40b-t0.0--falcon-40b-t0.0',\n",
       "       'gpt-3.5-turbo-t0.0--gpt-3.5-turbo-t0.0',\n",
       "       'gpt-3.5-turbo-t0.0--gpt-4-t0.0', 'gpt-4-t0.0--gpt-3.5-turbo-t0.0',\n",
       "       'gpt-4-t0.0--gpt-4-t0.0', 'koala-13b-t0.0--koala-13b-t0.0',\n",
       "       'luminous-supreme-t0.0--luminous-supreme-t0.0',\n",
       "       'oasst-12b-t0.0--oasst-12b-t0.0',\n",
       "       'text-davinci-003-t0.0--text-davinci-003-t0.0',\n",
       "       'vicuna-13b-t0.0--vicuna-13b-t0.0'],\n",
       "      dtype='object', name='models')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49944924",
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = v0.drop(axis=0, labels=['gpt-3.5-turbo-t0.0--gpt-4-t0.0', 'gpt-4-t0.0--gpt-3.5-turbo-t0.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b6a36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/3474250429.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  v0.index = v0.index.str.replace(r'(\\w*)-t0.0.*', r'\\1').str.lower()\n"
     ]
    }
   ],
   "source": [
    "v0.index = v0.index.str.replace(r'(\\w*)-t0.0.*', r'\\1').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c7df65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claude-v1.3', 'falcon-40b', 'gpt-3.5-turbo', 'gpt-4', 'koala-13b',\n",
       "       'luminous-supreme', 'oasst-12b', 'text-davinci-003', 'vicuna-13b'],\n",
       "      dtype='object', name='models')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f781f827",
   "metadata": {},
   "source": [
    "### Tracking the evolution of the benchmark, and of the models tested with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce2928f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 24), (41, 24), (55, 24))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0.shape, v1.shape, v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2184d",
   "metadata": {},
   "source": [
    "##### v2 = v1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511ee5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models\n",
       "gpt-4-turbo-2024-04-09            58.30\n",
       "gpt-4-0125-preview                52.50\n",
       "gpt-4-1106-preview                51.99\n",
       "gpt-4-0613                        51.09\n",
       "gpt-4o-2024-05-13                 48.34\n",
       "claude-3-opus-20240229            42.42\n",
       "gemini-1.5-pro-latest             41.72\n",
       "meta-llama-3-70b-instruct-hf      35.11\n",
       "llama-3-70b-instruct-anyscale     33.36\n",
       "claude-2.1                        32.50\n",
       "gemini-1.5-flash-latest           32.00\n",
       "claude-3-sonnet-20240229          30.53\n",
       "qwen1.5-72b-chat                  30.37\n",
       "mistral-large-2402                28.17\n",
       "gpt-3.5-turbo-0125                27.22\n",
       "gemini-1.0-pro                    26.95\n",
       "command-r-plus                    24.94\n",
       "openchat_3.5                      23.64\n",
       "claude-3-haiku-20240307           22.49\n",
       "sheep-duck-llama-2-70b-v1.1       21.50\n",
       "meta-llama-3-8b-instruct-hf       19.99\n",
       "llama-3-8b-instruct-anyscale      19.32\n",
       "openchat-3.5-1210                 18.22\n",
       "wizardlm-70b-v1.0                 17.40\n",
       "openchat-3.5-0106                 17.10\n",
       "qwen1.5-14b-chat                  16.80\n",
       "mistral-medium-2312               16.43\n",
       "qwen1.5-32b-chat                  15.41\n",
       "codegemma-7b-it                   15.30\n",
       "dolphin-2.5-mixtral-8x7b          15.10\n",
       "codellama-34b-instruct-hf         14.35\n",
       "command-r                         14.15\n",
       "gemma-1.1-7b-it                   14.14\n",
       "sus-chat-34b                      14.11\n",
       "mixtral-8x22b-instruct-v0.1       12.69\n",
       "tulu-2-dpo-70b                    12.62\n",
       "nous-hermes-2-mixtral-8x7b-sft    11.95\n",
       "wizardlm-13b-v1.2                 11.48\n",
       "vicuna-33b-v1.3                   11.27\n",
       "mistral-7b-instruct-v0.2           9.75\n",
       "yi-34b-chat                        8.27\n",
       "mixtral-8x7b-instruct-v0.1         8.17\n",
       "mistral-7b-instruct-v0.1           8.01\n",
       "yi-1.5-34b-chat                    7.67\n",
       "vicuna-13b-v1.5                    7.01\n",
       "yi-1.5-6b-chat                     6.73\n",
       "starling-lm-7b-beta                6.56\n",
       "sheep-duck-llama-2-13b             5.39\n",
       "yi-1.5-9b-chat                     4.37\n",
       "gemma-1.1-2b-it                    2.91\n",
       "qwen1.5-7b-chat                    2.58\n",
       "gemma-7b-it                        1.82\n",
       "llama-2-70b-chat-hf                0.81\n",
       "qwen1.5-0.5b-chat                  0.12\n",
       "qwen1.5-1.8b-chat                  0.00\n",
       "Name: -, clemscore, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.loc[:,colname_score].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665635a",
   "metadata": {},
   "source": [
    "Arrgh We now have double entries for llama-3, one with inference via anyscale, one with local inference via transformers....\n",
    "Need to drop the anyscale ones, and change the model names for the hf llama3s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d88ff10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = v2[~v2.index.str.contains('anyscale')].copy(deep=True)\n",
    "len(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "411ce6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/3568535097.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  v2.index = v2.index.str.replace(r'meta-(\\w*)', r'\\1')\n",
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/3568535097.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  v2.index = v2.index.str.replace(r'(\\w*)-hf', r'\\1')\n"
     ]
    }
   ],
   "source": [
    "v2.index = v2.index.str.replace(r'meta-(\\w*)', r'\\1')\n",
    "v2.index = v2.index.str.replace(r'(\\w*)-hf', r'\\1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd314147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['codellama-34b-instruct', 'llama-3-70b-instruct', 'llama-3-8b-instruct',\n",
       "       'mistral-7b-instruct-v0.1', 'mistral-7b-instruct-v0.2',\n",
       "       'mixtral-8x22b-instruct-v0.1', 'mixtral-8x7b-instruct-v0.1',\n",
       "       'nous-hermes-2-mixtral-8x7b-sft', 'qwen1.5-0.5b-chat',\n",
       "       'qwen1.5-1.8b-chat', 'qwen1.5-14b-chat', 'qwen1.5-32b-chat',\n",
       "       'qwen1.5-72b-chat', 'qwen1.5-7b-chat', 'sus-chat-34b',\n",
       "       'starling-lm-7b-beta', 'wizardlm-13b-v1.2', 'wizardlm-70b-v1.0',\n",
       "       'yi-1.5-34b-chat', 'yi-1.5-6b-chat', 'yi-1.5-9b-chat', 'yi-34b-chat',\n",
       "       'claude-2.1', 'claude-3-haiku-20240307', 'claude-3-opus-20240229',\n",
       "       'claude-3-sonnet-20240229', 'codegemma-7b-it', 'command-r-plus',\n",
       "       'command-r', 'dolphin-2.5-mixtral-8x7b', 'gemini-1.0-pro',\n",
       "       'gemini-1.5-flash-latest', 'gemini-1.5-pro-latest', 'gemma-1.1-2b-it',\n",
       "       'gemma-1.1-7b-it', 'gemma-7b-it', 'gpt-3.5-turbo-0125',\n",
       "       'gpt-4-0125-preview', 'gpt-4-0613', 'gpt-4-1106-preview',\n",
       "       'gpt-4-turbo-2024-04-09', 'gpt-4o-2024-05-13', 'llama-2-70b-chat',\n",
       "       'mistral-large-2402', 'mistral-medium-2312', 'openchat-3.5-0106',\n",
       "       'openchat-3.5-1210', 'openchat_3.5', 'sheep-duck-llama-2-13b',\n",
       "       'sheep-duck-llama-2-70b-v1.1', 'tulu-2-dpo-70b', 'vicuna-13b-v1.5',\n",
       "       'vicuna-33b-v1.3'],\n",
       "      dtype='object', name='models')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ea8701",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codellama-34b-instruct',\n",
       " 'llama-3-70b-instruct',\n",
       " 'llama-3-8b-instruct',\n",
       " 'mistral-7b-instruct-v0.1',\n",
       " 'mistral-7b-instruct-v0.2',\n",
       " 'mixtral-8x22b-instruct-v0.1',\n",
       " 'mixtral-8x7b-instruct-v0.1',\n",
       " 'nous-hermes-2-mixtral-8x7b-sft',\n",
       " 'qwen1.5-0.5b-chat',\n",
       " 'qwen1.5-1.8b-chat',\n",
       " 'qwen1.5-14b-chat',\n",
       " 'qwen1.5-32b-chat',\n",
       " 'qwen1.5-72b-chat',\n",
       " 'qwen1.5-7b-chat',\n",
       " 'sus-chat-34b',\n",
       " 'starling-lm-7b-beta',\n",
       " 'wizardlm-13b-v1.2',\n",
       " 'wizardlm-70b-v1.0',\n",
       " 'yi-1.5-34b-chat',\n",
       " 'yi-1.5-6b-chat',\n",
       " 'yi-1.5-9b-chat',\n",
       " 'yi-34b-chat',\n",
       " 'claude-2.1',\n",
       " 'claude-3-haiku-20240307',\n",
       " 'claude-3-opus-20240229',\n",
       " 'claude-3-sonnet-20240229',\n",
       " 'codegemma-7b-it',\n",
       " 'command-r-plus',\n",
       " 'command-r',\n",
       " 'dolphin-2.5-mixtral-8x7b',\n",
       " 'gemini-1.0-pro',\n",
       " 'gemini-1.5-flash-latest',\n",
       " 'gemini-1.5-pro-latest',\n",
       " 'gemma-1.1-2b-it',\n",
       " 'gemma-1.1-7b-it',\n",
       " 'gemma-7b-it',\n",
       " 'gpt-3.5-turbo-0125',\n",
       " 'gpt-4-0125-preview',\n",
       " 'gpt-4-0613',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt-4-turbo-2024-04-09',\n",
       " 'gpt-4o-2024-05-13',\n",
       " 'llama-2-70b-chat',\n",
       " 'mistral-large-2402',\n",
       " 'mistral-medium-2312',\n",
       " 'openchat-3.5-0106',\n",
       " 'openchat-3.5-1210',\n",
       " 'openchat_3.5',\n",
       " 'sheep-duck-llama-2-13b',\n",
       " 'sheep-duck-llama-2-70b-v1.1',\n",
       " 'tulu-2-dpo-70b',\n",
       " 'vicuna-13b-v1.5',\n",
       " 'vicuna-33b-v1.3']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1faced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_open = \\\n",
    "['codellama-34b-instruct',\n",
    " 'llama-3-70b-instruct',\n",
    " 'llama-3-8b-instruct',\n",
    " 'mistral-7b-instruct-v0.1',\n",
    " 'mistral-7b-instruct-v0.2',\n",
    " 'mixtral-8x22b-instruct-v0.1',\n",
    " 'mixtral-8x7b-instruct-v0.1',\n",
    " 'nous-hermes-2-mixtral-8x7b-sft',\n",
    " 'qwen1.5-0.5b-chat',\n",
    " 'qwen1.5-1.8b-chat',\n",
    " 'qwen1.5-14b-chat',\n",
    " 'qwen1.5-32b-chat',\n",
    " 'qwen1.5-72b-chat',\n",
    " 'qwen1.5-7b-chat',\n",
    " 'sus-chat-34b',\n",
    " 'starling-lm-7b-beta',\n",
    " 'wizardlm-13b-v1.2',\n",
    " 'wizardlm-70b-v1.0',\n",
    " 'yi-1.5-34b-chat',\n",
    " 'yi-1.5-6b-chat',\n",
    " 'yi-1.5-9b-chat',\n",
    " 'yi-34b-chat',\n",
    " 'codegemma-7b-it',\n",
    " 'command-r-plus',\n",
    " 'command-r',\n",
    " 'dolphin-2.5-mixtral-8x7b',\n",
    " 'gemma-1.1-2b-it',\n",
    " 'gemma-1.1-7b-it',\n",
    " 'gemma-7b-it',\n",
    " 'llama-2-70b-chat',\n",
    " 'openchat-3.5-0106',\n",
    " 'openchat-3.5-1210',\n",
    " 'openchat_3.5',\n",
    " 'sheep-duck-llama-2-13b',\n",
    " 'sheep-duck-llama-2-70b-v1.1',\n",
    " 'tulu-2-dpo-70b',\n",
    " 'vicuna-13b-v1.5',\n",
    " 'vicuna-33b-v1.3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7564c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_closed = set(v2.index.tolist()) - set(v2_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd1d5591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claude-2.1',\n",
       " 'claude-3-haiku-20240307',\n",
       " 'claude-3-opus-20240229',\n",
       " 'claude-3-sonnet-20240229',\n",
       " 'gemini-1.0-pro',\n",
       " 'gemini-1.5-flash-latest',\n",
       " 'gemini-1.5-pro-latest',\n",
       " 'gpt-3.5-turbo-0125',\n",
       " 'gpt-4-0125-preview',\n",
       " 'gpt-4-0613',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt-4-turbo-2024-04-09',\n",
       " 'gpt-4o-2024-05-13',\n",
       " 'mistral-large-2402',\n",
       " 'mistral-medium-2312'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2_closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f351427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2.loc[:, 'ow/gt'] = '🔓'\n",
    "# v2.loc[v2.index.isin(v2_closed), 'ow/gt'] = '🔑'\n",
    "\n",
    "## Maybe for screenshots for social media...\n",
    "#v2.loc[:, 'ow/gt'] = '🏋🏽‍♀️'\n",
    "#v2.loc[v2.index.isin(v2_closed), 'ow/gt'] = '🔒'#\n",
    "\n",
    "v2.loc[:, 'ow/gt'] = 'ow'\n",
    "v2.loc[v2.index.isin(v2_closed), 'ow/gt'] = '$$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "277b9a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-, clemscore</th>\n",
       "      <th>all, Average % Played</th>\n",
       "      <th>all, Average Quality Score</th>\n",
       "      <th>imagegame, % Played</th>\n",
       "      <th>imagegame, Quality Score</th>\n",
       "      <th>imagegame, Quality Score (std)</th>\n",
       "      <th>privateshared, % Played</th>\n",
       "      <th>privateshared, Quality Score</th>\n",
       "      <th>privateshared, Quality Score (std)</th>\n",
       "      <th>referencegame, % Played</th>\n",
       "      <th>referencegame, Quality Score</th>\n",
       "      <th>referencegame, Quality Score (std)</th>\n",
       "      <th>taboo, % Played</th>\n",
       "      <th>taboo, Quality Score</th>\n",
       "      <th>taboo, Quality Score (std)</th>\n",
       "      <th>wordle, % Played</th>\n",
       "      <th>wordle, Quality Score</th>\n",
       "      <th>wordle, Quality Score (std)</th>\n",
       "      <th>wordle_withclue, % Played</th>\n",
       "      <th>wordle_withclue, Quality Score</th>\n",
       "      <th>wordle_withclue, Quality Score (std)</th>\n",
       "      <th>wordle_withcritic, % Played</th>\n",
       "      <th>wordle_withcritic, Quality Score</th>\n",
       "      <th>wordle_withcritic, Quality Score (std)</th>\n",
       "      <th>ow/gt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codellama-34b-instruct</th>\n",
       "      <td>14.35</td>\n",
       "      <td>33.57</td>\n",
       "      <td>42.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>94.44</td>\n",
       "      <td>22.97</td>\n",
       "      <td>51.67</td>\n",
       "      <td>51.61</td>\n",
       "      <td>50.80</td>\n",
       "      <td>56.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.67</td>\n",
       "      <td>25.00</td>\n",
       "      <td>46.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-3-70b-instruct</th>\n",
       "      <td>35.11</td>\n",
       "      <td>80.72</td>\n",
       "      <td>43.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>84.37</td>\n",
       "      <td>13.69</td>\n",
       "      <td>100.00</td>\n",
       "      <td>64.44</td>\n",
       "      <td>48.00</td>\n",
       "      <td>91.67</td>\n",
       "      <td>70.30</td>\n",
       "      <td>39.37</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.81</td>\n",
       "      <td>96.67</td>\n",
       "      <td>14.37</td>\n",
       "      <td>32.34</td>\n",
       "      <td>86.67</td>\n",
       "      <td>25.64</td>\n",
       "      <td>39.08</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-3-8b-instruct</th>\n",
       "      <td>19.99</td>\n",
       "      <td>76.10</td>\n",
       "      <td>26.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "      <td>58.91</td>\n",
       "      <td>30.05</td>\n",
       "      <td>100.00</td>\n",
       "      <td>46.11</td>\n",
       "      <td>49.99</td>\n",
       "      <td>100.00</td>\n",
       "      <td>37.78</td>\n",
       "      <td>45.08</td>\n",
       "      <td>86.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>83.33</td>\n",
       "      <td>14.00</td>\n",
       "      <td>33.91</td>\n",
       "      <td>66.67</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.73</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-instruct-v0.1</th>\n",
       "      <td>8.01</td>\n",
       "      <td>37.14</td>\n",
       "      <td>21.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.58</td>\n",
       "      <td>100.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>49.89</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.67</td>\n",
       "      <td>45.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.67</td>\n",
       "      <td>20.00</td>\n",
       "      <td>44.72</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-instruct-v0.2</th>\n",
       "      <td>9.75</td>\n",
       "      <td>36.91</td>\n",
       "      <td>26.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>38.33</td>\n",
       "      <td>48.76</td>\n",
       "      <td>65.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.67</td>\n",
       "      <td>43.75</td>\n",
       "      <td>49.55</td>\n",
       "      <td>16.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixtral-8x22b-instruct-v0.1</th>\n",
       "      <td>12.69</td>\n",
       "      <td>52.14</td>\n",
       "      <td>24.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>36.67</td>\n",
       "      <td>48.32</td>\n",
       "      <td>58.33</td>\n",
       "      <td>40.00</td>\n",
       "      <td>49.71</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>33.30</td>\n",
       "      <td>50.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>41.40</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixtral-8x7b-instruct-v0.1</th>\n",
       "      <td>8.17</td>\n",
       "      <td>47.62</td>\n",
       "      <td>17.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.67</td>\n",
       "      <td>41.44</td>\n",
       "      <td>49.49</td>\n",
       "      <td>51.67</td>\n",
       "      <td>9.68</td>\n",
       "      <td>30.05</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>76.67</td>\n",
       "      <td>19.13</td>\n",
       "      <td>35.28</td>\n",
       "      <td>46.67</td>\n",
       "      <td>15.48</td>\n",
       "      <td>36.08</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nous-hermes-2-mixtral-8x7b-sft</th>\n",
       "      <td>11.95</td>\n",
       "      <td>39.68</td>\n",
       "      <td>30.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.78</td>\n",
       "      <td>36.93</td>\n",
       "      <td>48.40</td>\n",
       "      <td>93.33</td>\n",
       "      <td>47.92</td>\n",
       "      <td>47.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.33</td>\n",
       "      <td>15.62</td>\n",
       "      <td>30.10</td>\n",
       "      <td>33.33</td>\n",
       "      <td>20.00</td>\n",
       "      <td>42.16</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-0.5b-chat</th>\n",
       "      <td>0.12</td>\n",
       "      <td>25.72</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.67</td>\n",
       "      <td>1.92</td>\n",
       "      <td>13.87</td>\n",
       "      <td>46.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-1.8b-chat</th>\n",
       "      <td>0.00</td>\n",
       "      <td>15.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-14b-chat</th>\n",
       "      <td>16.80</td>\n",
       "      <td>40.95</td>\n",
       "      <td>41.02</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.58</td>\n",
       "      <td>14.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>44.44</td>\n",
       "      <td>49.83</td>\n",
       "      <td>46.67</td>\n",
       "      <td>41.07</td>\n",
       "      <td>47.25</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.67</td>\n",
       "      <td>40.00</td>\n",
       "      <td>54.77</td>\n",
       "      <td>3.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-32b-chat</th>\n",
       "      <td>15.41</td>\n",
       "      <td>63.69</td>\n",
       "      <td>24.19</td>\n",
       "      <td>67.5</td>\n",
       "      <td>42.15</td>\n",
       "      <td>29.29</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35.52</td>\n",
       "      <td>9.63</td>\n",
       "      <td>100.00</td>\n",
       "      <td>12.78</td>\n",
       "      <td>33.48</td>\n",
       "      <td>61.67</td>\n",
       "      <td>42.79</td>\n",
       "      <td>47.39</td>\n",
       "      <td>93.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>16.85</td>\n",
       "      <td>33.34</td>\n",
       "      <td>43.33</td>\n",
       "      <td>19.23</td>\n",
       "      <td>38.40</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-72b-chat</th>\n",
       "      <td>30.37</td>\n",
       "      <td>80.05</td>\n",
       "      <td>37.94</td>\n",
       "      <td>65.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>25.53</td>\n",
       "      <td>92.0</td>\n",
       "      <td>52.87</td>\n",
       "      <td>20.39</td>\n",
       "      <td>100.00</td>\n",
       "      <td>37.22</td>\n",
       "      <td>48.47</td>\n",
       "      <td>73.33</td>\n",
       "      <td>73.11</td>\n",
       "      <td>43.02</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>3.71</td>\n",
       "      <td>90.00</td>\n",
       "      <td>20.93</td>\n",
       "      <td>39.03</td>\n",
       "      <td>43.33</td>\n",
       "      <td>30.77</td>\n",
       "      <td>48.04</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-7b-chat</th>\n",
       "      <td>2.58</td>\n",
       "      <td>30.24</td>\n",
       "      <td>8.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>20.56</td>\n",
       "      <td>40.52</td>\n",
       "      <td>98.33</td>\n",
       "      <td>13.56</td>\n",
       "      <td>33.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sus-chat-34b</th>\n",
       "      <td>14.11</td>\n",
       "      <td>54.40</td>\n",
       "      <td>25.93</td>\n",
       "      <td>2.5</td>\n",
       "      <td>29.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>45.95</td>\n",
       "      <td>98.33</td>\n",
       "      <td>52.26</td>\n",
       "      <td>45.64</td>\n",
       "      <td>93.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.33</td>\n",
       "      <td>23.08</td>\n",
       "      <td>43.85</td>\n",
       "      <td>23.33</td>\n",
       "      <td>7.14</td>\n",
       "      <td>18.90</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starling-lm-7b-beta</th>\n",
       "      <td>6.56</td>\n",
       "      <td>30.89</td>\n",
       "      <td>21.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>97.12</td>\n",
       "      <td>4.08</td>\n",
       "      <td>62.22</td>\n",
       "      <td>30.36</td>\n",
       "      <td>46.19</td>\n",
       "      <td>46.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-13b-v1.2</th>\n",
       "      <td>11.48</td>\n",
       "      <td>39.57</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>10.11</td>\n",
       "      <td>21.96</td>\n",
       "      <td>100.00</td>\n",
       "      <td>71.11</td>\n",
       "      <td>45.45</td>\n",
       "      <td>35.00</td>\n",
       "      <td>64.29</td>\n",
       "      <td>45.12</td>\n",
       "      <td>26.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.33</td>\n",
       "      <td>6.25</td>\n",
       "      <td>25.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>22.22</td>\n",
       "      <td>40.37</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-70b-v1.0</th>\n",
       "      <td>17.40</td>\n",
       "      <td>46.19</td>\n",
       "      <td>37.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>81.67</td>\n",
       "      <td>38.80</td>\n",
       "      <td>56.67</td>\n",
       "      <td>70.59</td>\n",
       "      <td>44.58</td>\n",
       "      <td>73.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.67</td>\n",
       "      <td>17.84</td>\n",
       "      <td>34.09</td>\n",
       "      <td>36.67</td>\n",
       "      <td>18.18</td>\n",
       "      <td>40.45</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-1.5-34b-chat</th>\n",
       "      <td>7.67</td>\n",
       "      <td>52.38</td>\n",
       "      <td>14.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>43.33</td>\n",
       "      <td>49.69</td>\n",
       "      <td>66.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>18.25</td>\n",
       "      <td>36.48</td>\n",
       "      <td>33.33</td>\n",
       "      <td>11.67</td>\n",
       "      <td>31.48</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-1.5-6b-chat</th>\n",
       "      <td>6.73</td>\n",
       "      <td>41.43</td>\n",
       "      <td>16.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.33</td>\n",
       "      <td>34.59</td>\n",
       "      <td>47.72</td>\n",
       "      <td>65.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>86.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>20.00</td>\n",
       "      <td>42.16</td>\n",
       "      <td>16.67</td>\n",
       "      <td>26.67</td>\n",
       "      <td>43.46</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-1.5-9b-chat</th>\n",
       "      <td>4.37</td>\n",
       "      <td>38.10</td>\n",
       "      <td>11.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.67</td>\n",
       "      <td>41.94</td>\n",
       "      <td>49.61</td>\n",
       "      <td>41.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>86.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.67</td>\n",
       "      <td>7.14</td>\n",
       "      <td>26.73</td>\n",
       "      <td>40.00</td>\n",
       "      <td>8.33</td>\n",
       "      <td>28.87</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-34b-chat</th>\n",
       "      <td>8.27</td>\n",
       "      <td>40.86</td>\n",
       "      <td>20.25</td>\n",
       "      <td>35.0</td>\n",
       "      <td>9.07</td>\n",
       "      <td>10.84</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.02</td>\n",
       "      <td>17.17</td>\n",
       "      <td>3.33</td>\n",
       "      <td>33.33</td>\n",
       "      <td>51.64</td>\n",
       "      <td>68.33</td>\n",
       "      <td>41.46</td>\n",
       "      <td>49.88</td>\n",
       "      <td>83.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.33</td>\n",
       "      <td>26.92</td>\n",
       "      <td>43.85</td>\n",
       "      <td>26.67</td>\n",
       "      <td>22.92</td>\n",
       "      <td>36.66</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-2.1</th>\n",
       "      <td>32.50</td>\n",
       "      <td>82.14</td>\n",
       "      <td>39.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>74.92</td>\n",
       "      <td>26.26</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.56</td>\n",
       "      <td>50.14</td>\n",
       "      <td>95.00</td>\n",
       "      <td>64.91</td>\n",
       "      <td>45.93</td>\n",
       "      <td>96.67</td>\n",
       "      <td>7.59</td>\n",
       "      <td>21.16</td>\n",
       "      <td>86.67</td>\n",
       "      <td>21.60</td>\n",
       "      <td>39.58</td>\n",
       "      <td>96.67</td>\n",
       "      <td>17.82</td>\n",
       "      <td>35.34</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-haiku-20240307</th>\n",
       "      <td>22.49</td>\n",
       "      <td>79.52</td>\n",
       "      <td>28.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.46</td>\n",
       "      <td>34.83</td>\n",
       "      <td>100.00</td>\n",
       "      <td>17.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>63.33</td>\n",
       "      <td>78.95</td>\n",
       "      <td>32.11</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>8.44</td>\n",
       "      <td>21.27</td>\n",
       "      <td>93.33</td>\n",
       "      <td>14.58</td>\n",
       "      <td>31.64</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-opus-20240229</th>\n",
       "      <td>42.42</td>\n",
       "      <td>83.10</td>\n",
       "      <td>51.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.32</td>\n",
       "      <td>6.40</td>\n",
       "      <td>100.00</td>\n",
       "      <td>29.44</td>\n",
       "      <td>45.71</td>\n",
       "      <td>88.33</td>\n",
       "      <td>83.65</td>\n",
       "      <td>32.11</td>\n",
       "      <td>100.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>28.65</td>\n",
       "      <td>96.67</td>\n",
       "      <td>46.09</td>\n",
       "      <td>38.59</td>\n",
       "      <td>96.67</td>\n",
       "      <td>31.78</td>\n",
       "      <td>35.15</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-sonnet-20240229</th>\n",
       "      <td>30.53</td>\n",
       "      <td>85.24</td>\n",
       "      <td>35.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.81</td>\n",
       "      <td>25.28</td>\n",
       "      <td>100.00</td>\n",
       "      <td>27.22</td>\n",
       "      <td>44.63</td>\n",
       "      <td>100.00</td>\n",
       "      <td>73.61</td>\n",
       "      <td>36.73</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>23.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>33.65</td>\n",
       "      <td>96.67</td>\n",
       "      <td>22.13</td>\n",
       "      <td>33.35</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegemma-7b-it</th>\n",
       "      <td>15.30</td>\n",
       "      <td>51.95</td>\n",
       "      <td>29.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.67</td>\n",
       "      <td>96.60</td>\n",
       "      <td>18.19</td>\n",
       "      <td>83.33</td>\n",
       "      <td>26.00</td>\n",
       "      <td>44.31</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.33</td>\n",
       "      <td>14.10</td>\n",
       "      <td>30.31</td>\n",
       "      <td>16.67</td>\n",
       "      <td>40.00</td>\n",
       "      <td>54.77</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>command-r-plus</th>\n",
       "      <td>24.94</td>\n",
       "      <td>74.90</td>\n",
       "      <td>33.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.44</td>\n",
       "      <td>47.49</td>\n",
       "      <td>50.08</td>\n",
       "      <td>63.33</td>\n",
       "      <td>67.11</td>\n",
       "      <td>45.44</td>\n",
       "      <td>100.00</td>\n",
       "      <td>7.33</td>\n",
       "      <td>19.82</td>\n",
       "      <td>93.33</td>\n",
       "      <td>26.79</td>\n",
       "      <td>37.91</td>\n",
       "      <td>93.33</td>\n",
       "      <td>17.80</td>\n",
       "      <td>32.58</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>command-r</th>\n",
       "      <td>14.15</td>\n",
       "      <td>61.67</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>23.33</td>\n",
       "      <td>42.41</td>\n",
       "      <td>63.33</td>\n",
       "      <td>44.74</td>\n",
       "      <td>47.63</td>\n",
       "      <td>93.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>30.00</td>\n",
       "      <td>44.13</td>\n",
       "      <td>46.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>36.40</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dolphin-2.5-mixtral-8x7b</th>\n",
       "      <td>15.10</td>\n",
       "      <td>46.38</td>\n",
       "      <td>32.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.0</td>\n",
       "      <td>58.95</td>\n",
       "      <td>25.96</td>\n",
       "      <td>100.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>47.83</td>\n",
       "      <td>100.00</td>\n",
       "      <td>41.11</td>\n",
       "      <td>46.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.33</td>\n",
       "      <td>7.69</td>\n",
       "      <td>27.74</td>\n",
       "      <td>33.33</td>\n",
       "      <td>20.00</td>\n",
       "      <td>42.16</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-1.0-pro</th>\n",
       "      <td>26.95</td>\n",
       "      <td>80.14</td>\n",
       "      <td>33.63</td>\n",
       "      <td>30.0</td>\n",
       "      <td>49.08</td>\n",
       "      <td>26.50</td>\n",
       "      <td>76.0</td>\n",
       "      <td>63.70</td>\n",
       "      <td>19.97</td>\n",
       "      <td>100.00</td>\n",
       "      <td>46.11</td>\n",
       "      <td>49.99</td>\n",
       "      <td>85.00</td>\n",
       "      <td>55.23</td>\n",
       "      <td>44.53</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>3.85</td>\n",
       "      <td>86.67</td>\n",
       "      <td>12.82</td>\n",
       "      <td>32.76</td>\n",
       "      <td>93.33</td>\n",
       "      <td>7.74</td>\n",
       "      <td>21.98</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-1.5-flash-latest</th>\n",
       "      <td>32.00</td>\n",
       "      <td>76.14</td>\n",
       "      <td>42.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>78.18</td>\n",
       "      <td>20.17</td>\n",
       "      <td>100.00</td>\n",
       "      <td>61.11</td>\n",
       "      <td>48.89</td>\n",
       "      <td>91.67</td>\n",
       "      <td>57.88</td>\n",
       "      <td>43.61</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>3.71</td>\n",
       "      <td>66.67</td>\n",
       "      <td>33.33</td>\n",
       "      <td>38.90</td>\n",
       "      <td>80.00</td>\n",
       "      <td>20.97</td>\n",
       "      <td>31.07</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-1.5-pro-latest</th>\n",
       "      <td>41.72</td>\n",
       "      <td>82.14</td>\n",
       "      <td>50.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>84.15</td>\n",
       "      <td>11.58</td>\n",
       "      <td>100.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>47.83</td>\n",
       "      <td>85.00</td>\n",
       "      <td>70.59</td>\n",
       "      <td>35.84</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>22.43</td>\n",
       "      <td>93.33</td>\n",
       "      <td>41.37</td>\n",
       "      <td>39.25</td>\n",
       "      <td>96.67</td>\n",
       "      <td>32.99</td>\n",
       "      <td>35.32</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-1.1-2b-it</th>\n",
       "      <td>2.91</td>\n",
       "      <td>22.62</td>\n",
       "      <td>12.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>40.11</td>\n",
       "      <td>45.00</td>\n",
       "      <td>14.81</td>\n",
       "      <td>36.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.67</td>\n",
       "      <td>16.66</td>\n",
       "      <td>23.57</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-1.1-7b-it</th>\n",
       "      <td>14.14</td>\n",
       "      <td>49.67</td>\n",
       "      <td>28.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.83</td>\n",
       "      <td>10.10</td>\n",
       "      <td>100.00</td>\n",
       "      <td>92.22</td>\n",
       "      <td>26.86</td>\n",
       "      <td>35.00</td>\n",
       "      <td>52.38</td>\n",
       "      <td>51.18</td>\n",
       "      <td>73.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>76.67</td>\n",
       "      <td>6.52</td>\n",
       "      <td>22.88</td>\n",
       "      <td>56.67</td>\n",
       "      <td>8.82</td>\n",
       "      <td>26.43</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-7b-it</th>\n",
       "      <td>1.82</td>\n",
       "      <td>17.78</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.78</td>\n",
       "      <td>40.91</td>\n",
       "      <td>49.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0125</th>\n",
       "      <td>27.22</td>\n",
       "      <td>89.67</td>\n",
       "      <td>30.36</td>\n",
       "      <td>70.0</td>\n",
       "      <td>64.18</td>\n",
       "      <td>29.33</td>\n",
       "      <td>96.0</td>\n",
       "      <td>36.70</td>\n",
       "      <td>31.04</td>\n",
       "      <td>100.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>18.00</td>\n",
       "      <td>68.33</td>\n",
       "      <td>73.17</td>\n",
       "      <td>41.98</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96.67</td>\n",
       "      <td>24.25</td>\n",
       "      <td>40.95</td>\n",
       "      <td>96.67</td>\n",
       "      <td>10.92</td>\n",
       "      <td>27.56</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0125-preview</th>\n",
       "      <td>52.50</td>\n",
       "      <td>94.92</td>\n",
       "      <td>55.31</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.60</td>\n",
       "      <td>1.53</td>\n",
       "      <td>100.0</td>\n",
       "      <td>90.22</td>\n",
       "      <td>6.92</td>\n",
       "      <td>99.44</td>\n",
       "      <td>31.84</td>\n",
       "      <td>46.72</td>\n",
       "      <td>75.00</td>\n",
       "      <td>93.33</td>\n",
       "      <td>20.23</td>\n",
       "      <td>100.00</td>\n",
       "      <td>20.67</td>\n",
       "      <td>27.66</td>\n",
       "      <td>100.00</td>\n",
       "      <td>33.17</td>\n",
       "      <td>42.87</td>\n",
       "      <td>90.00</td>\n",
       "      <td>18.33</td>\n",
       "      <td>32.69</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0613</th>\n",
       "      <td>51.09</td>\n",
       "      <td>94.88</td>\n",
       "      <td>53.85</td>\n",
       "      <td>77.5</td>\n",
       "      <td>98.19</td>\n",
       "      <td>10.06</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.33</td>\n",
       "      <td>4.12</td>\n",
       "      <td>100.00</td>\n",
       "      <td>35.56</td>\n",
       "      <td>48.00</td>\n",
       "      <td>86.67</td>\n",
       "      <td>79.81</td>\n",
       "      <td>33.22</td>\n",
       "      <td>100.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>25.78</td>\n",
       "      <td>100.00</td>\n",
       "      <td>36.78</td>\n",
       "      <td>40.40</td>\n",
       "      <td>100.00</td>\n",
       "      <td>20.28</td>\n",
       "      <td>29.17</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-1106-preview</th>\n",
       "      <td>51.99</td>\n",
       "      <td>98.10</td>\n",
       "      <td>53.00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.34</td>\n",
       "      <td>10.24</td>\n",
       "      <td>100.0</td>\n",
       "      <td>87.08</td>\n",
       "      <td>10.69</td>\n",
       "      <td>100.00</td>\n",
       "      <td>29.44</td>\n",
       "      <td>45.71</td>\n",
       "      <td>91.67</td>\n",
       "      <td>83.94</td>\n",
       "      <td>29.57</td>\n",
       "      <td>100.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>27.56</td>\n",
       "      <td>100.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>39.53</td>\n",
       "      <td>100.00</td>\n",
       "      <td>34.22</td>\n",
       "      <td>39.55</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo-2024-04-09</th>\n",
       "      <td>58.30</td>\n",
       "      <td>94.88</td>\n",
       "      <td>61.45</td>\n",
       "      <td>82.5</td>\n",
       "      <td>99.79</td>\n",
       "      <td>1.22</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.68</td>\n",
       "      <td>9.45</td>\n",
       "      <td>100.00</td>\n",
       "      <td>88.89</td>\n",
       "      <td>31.51</td>\n",
       "      <td>85.00</td>\n",
       "      <td>82.35</td>\n",
       "      <td>30.81</td>\n",
       "      <td>100.00</td>\n",
       "      <td>16.33</td>\n",
       "      <td>31.35</td>\n",
       "      <td>100.00</td>\n",
       "      <td>29.89</td>\n",
       "      <td>39.18</td>\n",
       "      <td>96.67</td>\n",
       "      <td>20.23</td>\n",
       "      <td>28.21</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o-2024-05-13</th>\n",
       "      <td>48.34</td>\n",
       "      <td>85.71</td>\n",
       "      <td>56.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>94.66</td>\n",
       "      <td>5.56</td>\n",
       "      <td>100.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>30.08</td>\n",
       "      <td>100.00</td>\n",
       "      <td>75.28</td>\n",
       "      <td>35.19</td>\n",
       "      <td>100.00</td>\n",
       "      <td>19.33</td>\n",
       "      <td>28.52</td>\n",
       "      <td>100.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>36.38</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.11</td>\n",
       "      <td>33.14</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-70b-chat</th>\n",
       "      <td>0.81</td>\n",
       "      <td>7.14</td>\n",
       "      <td>11.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.67</td>\n",
       "      <td>22.62</td>\n",
       "      <td>42.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-large-2402</th>\n",
       "      <td>28.17</td>\n",
       "      <td>66.86</td>\n",
       "      <td>42.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>77.07</td>\n",
       "      <td>27.28</td>\n",
       "      <td>100.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>43.42</td>\n",
       "      <td>60.00</td>\n",
       "      <td>88.89</td>\n",
       "      <td>31.87</td>\n",
       "      <td>63.33</td>\n",
       "      <td>5.26</td>\n",
       "      <td>22.94</td>\n",
       "      <td>83.33</td>\n",
       "      <td>26.80</td>\n",
       "      <td>36.21</td>\n",
       "      <td>63.33</td>\n",
       "      <td>29.82</td>\n",
       "      <td>40.57</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-medium-2312</th>\n",
       "      <td>16.43</td>\n",
       "      <td>49.25</td>\n",
       "      <td>33.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>15.28</td>\n",
       "      <td>24.12</td>\n",
       "      <td>76.11</td>\n",
       "      <td>48.91</td>\n",
       "      <td>50.17</td>\n",
       "      <td>30.00</td>\n",
       "      <td>88.89</td>\n",
       "      <td>32.34</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>83.33</td>\n",
       "      <td>26.80</td>\n",
       "      <td>42.67</td>\n",
       "      <td>53.33</td>\n",
       "      <td>20.31</td>\n",
       "      <td>40.02</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat-3.5-0106</th>\n",
       "      <td>17.10</td>\n",
       "      <td>52.57</td>\n",
       "      <td>32.52</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.21</td>\n",
       "      <td>98.0</td>\n",
       "      <td>56.86</td>\n",
       "      <td>23.59</td>\n",
       "      <td>100.00</td>\n",
       "      <td>93.33</td>\n",
       "      <td>25.01</td>\n",
       "      <td>65.00</td>\n",
       "      <td>64.10</td>\n",
       "      <td>48.60</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>12.50</td>\n",
       "      <td>31.08</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat-3.5-1210</th>\n",
       "      <td>18.22</td>\n",
       "      <td>51.19</td>\n",
       "      <td>35.60</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.17</td>\n",
       "      <td>7.76</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.82</td>\n",
       "      <td>24.23</td>\n",
       "      <td>100.00</td>\n",
       "      <td>90.56</td>\n",
       "      <td>29.33</td>\n",
       "      <td>40.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>48.15</td>\n",
       "      <td>46.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.33</td>\n",
       "      <td>20.83</td>\n",
       "      <td>40.14</td>\n",
       "      <td>23.33</td>\n",
       "      <td>7.14</td>\n",
       "      <td>18.90</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat_3.5</th>\n",
       "      <td>23.64</td>\n",
       "      <td>63.52</td>\n",
       "      <td>37.22</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8.70</td>\n",
       "      <td>11.65</td>\n",
       "      <td>38.0</td>\n",
       "      <td>73.36</td>\n",
       "      <td>22.12</td>\n",
       "      <td>100.00</td>\n",
       "      <td>73.89</td>\n",
       "      <td>44.05</td>\n",
       "      <td>100.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>49.32</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.67</td>\n",
       "      <td>15.15</td>\n",
       "      <td>31.14</td>\n",
       "      <td>30.00</td>\n",
       "      <td>44.44</td>\n",
       "      <td>52.70</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sheep-duck-llama-2-13b</th>\n",
       "      <td>5.39</td>\n",
       "      <td>31.90</td>\n",
       "      <td>16.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.67</td>\n",
       "      <td>1.72</td>\n",
       "      <td>13.05</td>\n",
       "      <td>83.33</td>\n",
       "      <td>4.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.33</td>\n",
       "      <td>28.57</td>\n",
       "      <td>48.80</td>\n",
       "      <td>20.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>51.64</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sheep-duck-llama-2-70b-v1.1</th>\n",
       "      <td>21.50</td>\n",
       "      <td>41.19</td>\n",
       "      <td>52.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>83.33</td>\n",
       "      <td>37.37</td>\n",
       "      <td>55.00</td>\n",
       "      <td>90.91</td>\n",
       "      <td>29.19</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.33</td>\n",
       "      <td>42.31</td>\n",
       "      <td>44.94</td>\n",
       "      <td>30.00</td>\n",
       "      <td>44.44</td>\n",
       "      <td>46.40</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tulu-2-dpo-70b</th>\n",
       "      <td>12.62</td>\n",
       "      <td>49.76</td>\n",
       "      <td>25.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>16.67</td>\n",
       "      <td>37.37</td>\n",
       "      <td>68.33</td>\n",
       "      <td>68.29</td>\n",
       "      <td>47.11</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.33</td>\n",
       "      <td>16.88</td>\n",
       "      <td>29.83</td>\n",
       "      <td>46.67</td>\n",
       "      <td>25.00</td>\n",
       "      <td>42.74</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.5</th>\n",
       "      <td>7.01</td>\n",
       "      <td>39.52</td>\n",
       "      <td>17.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.27</td>\n",
       "      <td>8.84</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.67</td>\n",
       "      <td>60.71</td>\n",
       "      <td>49.73</td>\n",
       "      <td>53.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.67</td>\n",
       "      <td>21.21</td>\n",
       "      <td>40.20</td>\n",
       "      <td>20.00</td>\n",
       "      <td>4.17</td>\n",
       "      <td>10.21</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-33b-v1.3</th>\n",
       "      <td>11.27</td>\n",
       "      <td>23.81</td>\n",
       "      <td>47.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.67</td>\n",
       "      <td>89.29</td>\n",
       "      <td>31.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.00</td>\n",
       "      <td>16.67</td>\n",
       "      <td>28.87</td>\n",
       "      <td>10.00</td>\n",
       "      <td>83.33</td>\n",
       "      <td>28.87</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                -, clemscore  all, Average % Played  \\\n",
       "models                                                                \n",
       "codellama-34b-instruct                 14.35                  33.57   \n",
       "llama-3-70b-instruct                   35.11                  80.72   \n",
       "llama-3-8b-instruct                    19.99                  76.10   \n",
       "mistral-7b-instruct-v0.1                8.01                  37.14   \n",
       "mistral-7b-instruct-v0.2                9.75                  36.91   \n",
       "mixtral-8x22b-instruct-v0.1            12.69                  52.14   \n",
       "mixtral-8x7b-instruct-v0.1              8.17                  47.62   \n",
       "nous-hermes-2-mixtral-8x7b-sft         11.95                  39.68   \n",
       "qwen1.5-0.5b-chat                       0.12                  25.72   \n",
       "qwen1.5-1.8b-chat                       0.00                  15.24   \n",
       "qwen1.5-14b-chat                       16.80                  40.95   \n",
       "qwen1.5-32b-chat                       15.41                  63.69   \n",
       "qwen1.5-72b-chat                       30.37                  80.05   \n",
       "qwen1.5-7b-chat                         2.58                  30.24   \n",
       "sus-chat-34b                           14.11                  54.40   \n",
       "starling-lm-7b-beta                     6.56                  30.89   \n",
       "wizardlm-13b-v1.2                      11.48                  39.57   \n",
       "wizardlm-70b-v1.0                      17.40                  46.19   \n",
       "yi-1.5-34b-chat                         7.67                  52.38   \n",
       "yi-1.5-6b-chat                          6.73                  41.43   \n",
       "yi-1.5-9b-chat                          4.37                  38.10   \n",
       "yi-34b-chat                             8.27                  40.86   \n",
       "claude-2.1                             32.50                  82.14   \n",
       "claude-3-haiku-20240307                22.49                  79.52   \n",
       "claude-3-opus-20240229                 42.42                  83.10   \n",
       "claude-3-sonnet-20240229               30.53                  85.24   \n",
       "codegemma-7b-it                        15.30                  51.95   \n",
       "command-r-plus                         24.94                  74.90   \n",
       "command-r                              14.15                  61.67   \n",
       "dolphin-2.5-mixtral-8x7b               15.10                  46.38   \n",
       "gemini-1.0-pro                         26.95                  80.14   \n",
       "gemini-1.5-flash-latest                32.00                  76.14   \n",
       "gemini-1.5-pro-latest                  41.72                  82.14   \n",
       "gemma-1.1-2b-it                         2.91                  22.62   \n",
       "gemma-1.1-7b-it                        14.14                  49.67   \n",
       "gemma-7b-it                             1.82                  17.78   \n",
       "gpt-3.5-turbo-0125                     27.22                  89.67   \n",
       "gpt-4-0125-preview                     52.50                  94.92   \n",
       "gpt-4-0613                             51.09                  94.88   \n",
       "gpt-4-1106-preview                     51.99                  98.10   \n",
       "gpt-4-turbo-2024-04-09                 58.30                  94.88   \n",
       "gpt-4o-2024-05-13                      48.34                  85.71   \n",
       "llama-2-70b-chat                        0.81                   7.14   \n",
       "mistral-large-2402                     28.17                  66.86   \n",
       "mistral-medium-2312                    16.43                  49.25   \n",
       "openchat-3.5-0106                      17.10                  52.57   \n",
       "openchat-3.5-1210                      18.22                  51.19   \n",
       "openchat_3.5                           23.64                  63.52   \n",
       "sheep-duck-llama-2-13b                  5.39                  31.90   \n",
       "sheep-duck-llama-2-70b-v1.1            21.50                  41.19   \n",
       "tulu-2-dpo-70b                         12.62                  49.76   \n",
       "vicuna-13b-v1.5                         7.01                  39.52   \n",
       "vicuna-33b-v1.3                        11.27                  23.81   \n",
       "\n",
       "                                all, Average Quality Score  \\\n",
       "models                                                       \n",
       "codellama-34b-instruct                               42.76   \n",
       "llama-3-70b-instruct                                 43.50   \n",
       "llama-3-8b-instruct                                  26.27   \n",
       "mistral-7b-instruct-v0.1                             21.58   \n",
       "mistral-7b-instruct-v0.2                             26.42   \n",
       "mixtral-8x22b-instruct-v0.1                          24.33   \n",
       "mixtral-8x7b-instruct-v0.1                           17.15   \n",
       "nous-hermes-2-mixtral-8x7b-sft                       30.12   \n",
       "qwen1.5-0.5b-chat                                     0.48   \n",
       "qwen1.5-1.8b-chat                                     0.00   \n",
       "qwen1.5-14b-chat                                     41.02   \n",
       "qwen1.5-32b-chat                                     24.19   \n",
       "qwen1.5-72b-chat                                     37.94   \n",
       "qwen1.5-7b-chat                                       8.53   \n",
       "sus-chat-34b                                         25.93   \n",
       "starling-lm-7b-beta                                  21.25   \n",
       "wizardlm-13b-v1.2                                    29.00   \n",
       "wizardlm-70b-v1.0                                    37.66   \n",
       "yi-1.5-34b-chat                                      14.65   \n",
       "yi-1.5-6b-chat                                       16.25   \n",
       "yi-1.5-9b-chat                                       11.48   \n",
       "yi-34b-chat                                          20.25   \n",
       "claude-2.1                                           39.57   \n",
       "claude-3-haiku-20240307                              28.28   \n",
       "claude-3-opus-20240229                               51.05   \n",
       "claude-3-sonnet-20240229                             35.82   \n",
       "codegemma-7b-it                                      29.45   \n",
       "command-r-plus                                       33.30   \n",
       "command-r                                            22.95   \n",
       "dolphin-2.5-mixtral-8x7b                             32.55   \n",
       "gemini-1.0-pro                                       33.63   \n",
       "gemini-1.5-flash-latest                              42.03   \n",
       "gemini-1.5-pro-latest                                50.79   \n",
       "gemma-1.1-2b-it                                      12.87   \n",
       "gemma-1.1-7b-it                                      28.46   \n",
       "gemma-7b-it                                          10.23   \n",
       "gpt-3.5-turbo-0125                                   30.36   \n",
       "gpt-4-0125-preview                                   55.31   \n",
       "gpt-4-0613                                           53.85   \n",
       "gpt-4-1106-preview                                   53.00   \n",
       "gpt-4-turbo-2024-04-09                               61.45   \n",
       "gpt-4o-2024-05-13                                    56.40   \n",
       "llama-2-70b-chat                                     11.31   \n",
       "mistral-large-2402                                   42.14   \n",
       "mistral-medium-2312                                  33.36   \n",
       "openchat-3.5-0106                                    32.52   \n",
       "openchat-3.5-1210                                    35.60   \n",
       "openchat_3.5                                         37.22   \n",
       "sheep-duck-llama-2-13b                               16.90   \n",
       "sheep-duck-llama-2-70b-v1.1                          52.20   \n",
       "tulu-2-dpo-70b                                       25.37   \n",
       "vicuna-13b-v1.5                                      17.73   \n",
       "vicuna-33b-v1.3                                      47.32   \n",
       "\n",
       "                                imagegame, % Played  imagegame, Quality Score  \\\n",
       "models                                                                          \n",
       "codellama-34b-instruct                          0.0                       NaN   \n",
       "llama-3-70b-instruct                            0.0                       NaN   \n",
       "llama-3-8b-instruct                             0.0                       NaN   \n",
       "mistral-7b-instruct-v0.1                        0.0                       NaN   \n",
       "mistral-7b-instruct-v0.2                        0.0                       NaN   \n",
       "mixtral-8x22b-instruct-v0.1                     0.0                       NaN   \n",
       "mixtral-8x7b-instruct-v0.1                      0.0                       NaN   \n",
       "nous-hermes-2-mixtral-8x7b-sft                  0.0                       NaN   \n",
       "qwen1.5-0.5b-chat                               0.0                       NaN   \n",
       "qwen1.5-1.8b-chat                               0.0                       NaN   \n",
       "qwen1.5-14b-chat                               30.0                     20.58   \n",
       "qwen1.5-32b-chat                               67.5                     42.15   \n",
       "qwen1.5-72b-chat                               65.0                     50.00   \n",
       "qwen1.5-7b-chat                                 0.0                       NaN   \n",
       "sus-chat-34b                                    2.5                     29.00   \n",
       "starling-lm-7b-beta                             0.0                       NaN   \n",
       "wizardlm-13b-v1.2                               0.0                       NaN   \n",
       "wizardlm-70b-v1.0                               0.0                       NaN   \n",
       "yi-1.5-34b-chat                                 0.0                       NaN   \n",
       "yi-1.5-6b-chat                                  0.0                       NaN   \n",
       "yi-1.5-9b-chat                                  0.0                       NaN   \n",
       "yi-34b-chat                                    35.0                      9.07   \n",
       "claude-2.1                                      0.0                       NaN   \n",
       "claude-3-haiku-20240307                         0.0                       NaN   \n",
       "claude-3-opus-20240229                          0.0                       NaN   \n",
       "claude-3-sonnet-20240229                        0.0                       NaN   \n",
       "codegemma-7b-it                                 0.0                       NaN   \n",
       "command-r-plus                                  0.0                       NaN   \n",
       "command-r                                       0.0                       NaN   \n",
       "dolphin-2.5-mixtral-8x7b                        0.0                       NaN   \n",
       "gemini-1.0-pro                                 30.0                     49.08   \n",
       "gemini-1.5-flash-latest                         0.0                       NaN   \n",
       "gemini-1.5-pro-latest                           0.0                       NaN   \n",
       "gemma-1.1-2b-it                                 0.0                       NaN   \n",
       "gemma-1.1-7b-it                                 0.0                       NaN   \n",
       "gemma-7b-it                                     0.0                       NaN   \n",
       "gpt-3.5-turbo-0125                             70.0                     64.18   \n",
       "gpt-4-0125-preview                            100.0                     99.60   \n",
       "gpt-4-0613                                     77.5                     98.19   \n",
       "gpt-4-1106-preview                             95.0                     94.34   \n",
       "gpt-4-turbo-2024-04-09                         82.5                     99.79   \n",
       "gpt-4o-2024-05-13                               0.0                       NaN   \n",
       "llama-2-70b-chat                                0.0                       NaN   \n",
       "mistral-large-2402                              0.0                       NaN   \n",
       "mistral-medium-2312                             0.0                       NaN   \n",
       "openchat-3.5-0106                              35.0                      0.86   \n",
       "openchat-3.5-1210                              15.0                      3.17   \n",
       "openchat_3.5                                   50.0                      8.70   \n",
       "sheep-duck-llama-2-13b                          0.0                       NaN   \n",
       "sheep-duck-llama-2-70b-v1.1                     0.0                       NaN   \n",
       "tulu-2-dpo-70b                                  0.0                       NaN   \n",
       "vicuna-13b-v1.5                                 0.0                       NaN   \n",
       "vicuna-33b-v1.3                                 0.0                       NaN   \n",
       "\n",
       "                                imagegame, Quality Score (std)  \\\n",
       "models                                                           \n",
       "codellama-34b-instruct                                     NaN   \n",
       "llama-3-70b-instruct                                       NaN   \n",
       "llama-3-8b-instruct                                        NaN   \n",
       "mistral-7b-instruct-v0.1                                   NaN   \n",
       "mistral-7b-instruct-v0.2                                   NaN   \n",
       "mixtral-8x22b-instruct-v0.1                                NaN   \n",
       "mixtral-8x7b-instruct-v0.1                                 NaN   \n",
       "nous-hermes-2-mixtral-8x7b-sft                             NaN   \n",
       "qwen1.5-0.5b-chat                                          NaN   \n",
       "qwen1.5-1.8b-chat                                          NaN   \n",
       "qwen1.5-14b-chat                                         14.69   \n",
       "qwen1.5-32b-chat                                         29.29   \n",
       "qwen1.5-72b-chat                                         25.53   \n",
       "qwen1.5-7b-chat                                            NaN   \n",
       "sus-chat-34b                                               NaN   \n",
       "starling-lm-7b-beta                                        NaN   \n",
       "wizardlm-13b-v1.2                                          NaN   \n",
       "wizardlm-70b-v1.0                                          NaN   \n",
       "yi-1.5-34b-chat                                            NaN   \n",
       "yi-1.5-6b-chat                                             NaN   \n",
       "yi-1.5-9b-chat                                             NaN   \n",
       "yi-34b-chat                                              10.84   \n",
       "claude-2.1                                                 NaN   \n",
       "claude-3-haiku-20240307                                    NaN   \n",
       "claude-3-opus-20240229                                     NaN   \n",
       "claude-3-sonnet-20240229                                   NaN   \n",
       "codegemma-7b-it                                            NaN   \n",
       "command-r-plus                                             NaN   \n",
       "command-r                                                  NaN   \n",
       "dolphin-2.5-mixtral-8x7b                                   NaN   \n",
       "gemini-1.0-pro                                           26.50   \n",
       "gemini-1.5-flash-latest                                    NaN   \n",
       "gemini-1.5-pro-latest                                      NaN   \n",
       "gemma-1.1-2b-it                                            NaN   \n",
       "gemma-1.1-7b-it                                            NaN   \n",
       "gemma-7b-it                                                NaN   \n",
       "gpt-3.5-turbo-0125                                       29.33   \n",
       "gpt-4-0125-preview                                        1.53   \n",
       "gpt-4-0613                                               10.06   \n",
       "gpt-4-1106-preview                                       10.24   \n",
       "gpt-4-turbo-2024-04-09                                    1.22   \n",
       "gpt-4o-2024-05-13                                          NaN   \n",
       "llama-2-70b-chat                                           NaN   \n",
       "mistral-large-2402                                         NaN   \n",
       "mistral-medium-2312                                        NaN   \n",
       "openchat-3.5-0106                                         3.21   \n",
       "openchat-3.5-1210                                         7.76   \n",
       "openchat_3.5                                             11.65   \n",
       "sheep-duck-llama-2-13b                                     NaN   \n",
       "sheep-duck-llama-2-70b-v1.1                                NaN   \n",
       "tulu-2-dpo-70b                                             NaN   \n",
       "vicuna-13b-v1.5                                            NaN   \n",
       "vicuna-33b-v1.3                                            NaN   \n",
       "\n",
       "                                privateshared, % Played  \\\n",
       "models                                                    \n",
       "codellama-34b-instruct                              0.0   \n",
       "llama-3-70b-instruct                              100.0   \n",
       "llama-3-8b-instruct                                96.0   \n",
       "mistral-7b-instruct-v0.1                           20.0   \n",
       "mistral-7b-instruct-v0.2                            0.0   \n",
       "mixtral-8x22b-instruct-v0.1                         0.0   \n",
       "mixtral-8x7b-instruct-v0.1                          0.0   \n",
       "nous-hermes-2-mixtral-8x7b-sft                      0.0   \n",
       "qwen1.5-0.5b-chat                                   0.0   \n",
       "qwen1.5-1.8b-chat                                   0.0   \n",
       "qwen1.5-14b-chat                                    0.0   \n",
       "qwen1.5-32b-chat                                   20.0   \n",
       "qwen1.5-72b-chat                                   92.0   \n",
       "qwen1.5-7b-chat                                     0.0   \n",
       "sus-chat-34b                                       20.0   \n",
       "starling-lm-7b-beta                                 4.0   \n",
       "wizardlm-13b-v1.2                                  42.0   \n",
       "wizardlm-70b-v1.0                                   0.0   \n",
       "yi-1.5-34b-chat                                     0.0   \n",
       "yi-1.5-6b-chat                                      0.0   \n",
       "yi-1.5-9b-chat                                      0.0   \n",
       "yi-34b-chat                                        26.0   \n",
       "claude-2.1                                        100.0   \n",
       "claude-3-haiku-20240307                           100.0   \n",
       "claude-3-opus-20240229                            100.0   \n",
       "claude-3-sonnet-20240229                          100.0   \n",
       "codegemma-7b-it                                    42.0   \n",
       "command-r-plus                                      NaN   \n",
       "command-r                                           NaN   \n",
       "dolphin-2.5-mixtral-8x7b                           48.0   \n",
       "gemini-1.0-pro                                     76.0   \n",
       "gemini-1.5-flash-latest                            98.0   \n",
       "gemini-1.5-pro-latest                             100.0   \n",
       "gemma-1.1-2b-it                                     0.0   \n",
       "gemma-1.1-7b-it                                     6.0   \n",
       "gemma-7b-it                                         0.0   \n",
       "gpt-3.5-turbo-0125                                 96.0   \n",
       "gpt-4-0125-preview                                100.0   \n",
       "gpt-4-0613                                        100.0   \n",
       "gpt-4-1106-preview                                100.0   \n",
       "gpt-4-turbo-2024-04-09                            100.0   \n",
       "gpt-4o-2024-05-13                                 100.0   \n",
       "llama-2-70b-chat                                    0.0   \n",
       "mistral-large-2402                                 98.0   \n",
       "mistral-medium-2312                                22.0   \n",
       "openchat-3.5-0106                                  98.0   \n",
       "openchat-3.5-1210                                  80.0   \n",
       "openchat_3.5                                       38.0   \n",
       "sheep-duck-llama-2-13b                              0.0   \n",
       "sheep-duck-llama-2-70b-v1.1                         0.0   \n",
       "tulu-2-dpo-70b                                      0.0   \n",
       "vicuna-13b-v1.5                                    20.0   \n",
       "vicuna-33b-v1.3                                     0.0   \n",
       "\n",
       "                                privateshared, Quality Score  \\\n",
       "models                                                         \n",
       "codellama-34b-instruct                                   NaN   \n",
       "llama-3-70b-instruct                                   84.37   \n",
       "llama-3-8b-instruct                                    58.91   \n",
       "mistral-7b-instruct-v0.1                                1.21   \n",
       "mistral-7b-instruct-v0.2                                 NaN   \n",
       "mixtral-8x22b-instruct-v0.1                              NaN   \n",
       "mixtral-8x7b-instruct-v0.1                               NaN   \n",
       "nous-hermes-2-mixtral-8x7b-sft                           NaN   \n",
       "qwen1.5-0.5b-chat                                        NaN   \n",
       "qwen1.5-1.8b-chat                                        NaN   \n",
       "qwen1.5-14b-chat                                         NaN   \n",
       "qwen1.5-32b-chat                                       35.52   \n",
       "qwen1.5-72b-chat                                       52.87   \n",
       "qwen1.5-7b-chat                                          NaN   \n",
       "sus-chat-34b                                            0.00   \n",
       "starling-lm-7b-beta                                    97.12   \n",
       "wizardlm-13b-v1.2                                      10.11   \n",
       "wizardlm-70b-v1.0                                        NaN   \n",
       "yi-1.5-34b-chat                                          NaN   \n",
       "yi-1.5-6b-chat                                           NaN   \n",
       "yi-1.5-9b-chat                                           NaN   \n",
       "yi-34b-chat                                             8.02   \n",
       "claude-2.1                                             74.92   \n",
       "claude-3-haiku-20240307                                50.46   \n",
       "claude-3-opus-20240229                                 95.32   \n",
       "claude-3-sonnet-20240229                               60.81   \n",
       "codegemma-7b-it                                         0.00   \n",
       "command-r-plus                                           NaN   \n",
       "command-r                                                NaN   \n",
       "dolphin-2.5-mixtral-8x7b                               58.95   \n",
       "gemini-1.0-pro                                         63.70   \n",
       "gemini-1.5-flash-latest                                78.18   \n",
       "gemini-1.5-pro-latest                                  84.15   \n",
       "gemma-1.1-2b-it                                          NaN   \n",
       "gemma-1.1-7b-it                                        10.83   \n",
       "gemma-7b-it                                              NaN   \n",
       "gpt-3.5-turbo-0125                                     36.70   \n",
       "gpt-4-0125-preview                                     90.22   \n",
       "gpt-4-0613                                             97.33   \n",
       "gpt-4-1106-preview                                     87.08   \n",
       "gpt-4-turbo-2024-04-09                                 92.68   \n",
       "gpt-4o-2024-05-13                                      94.66   \n",
       "llama-2-70b-chat                                         NaN   \n",
       "mistral-large-2402                                     77.07   \n",
       "mistral-medium-2312                                    15.28   \n",
       "openchat-3.5-0106                                      56.86   \n",
       "openchat-3.5-1210                                      60.82   \n",
       "openchat_3.5                                           73.36   \n",
       "sheep-duck-llama-2-13b                                   NaN   \n",
       "sheep-duck-llama-2-70b-v1.1                              NaN   \n",
       "tulu-2-dpo-70b                                           NaN   \n",
       "vicuna-13b-v1.5                                        20.27   \n",
       "vicuna-33b-v1.3                                          NaN   \n",
       "\n",
       "                                privateshared, Quality Score (std)  \\\n",
       "models                                                               \n",
       "codellama-34b-instruct                                         NaN   \n",
       "llama-3-70b-instruct                                         13.69   \n",
       "llama-3-8b-instruct                                          30.05   \n",
       "mistral-7b-instruct-v0.1                                      2.58   \n",
       "mistral-7b-instruct-v0.2                                       NaN   \n",
       "mixtral-8x22b-instruct-v0.1                                    NaN   \n",
       "mixtral-8x7b-instruct-v0.1                                     NaN   \n",
       "nous-hermes-2-mixtral-8x7b-sft                                 NaN   \n",
       "qwen1.5-0.5b-chat                                              NaN   \n",
       "qwen1.5-1.8b-chat                                              NaN   \n",
       "qwen1.5-14b-chat                                               NaN   \n",
       "qwen1.5-32b-chat                                              9.63   \n",
       "qwen1.5-72b-chat                                             20.39   \n",
       "qwen1.5-7b-chat                                                NaN   \n",
       "sus-chat-34b                                                  0.00   \n",
       "starling-lm-7b-beta                                           4.08   \n",
       "wizardlm-13b-v1.2                                            21.96   \n",
       "wizardlm-70b-v1.0                                              NaN   \n",
       "yi-1.5-34b-chat                                                NaN   \n",
       "yi-1.5-6b-chat                                                 NaN   \n",
       "yi-1.5-9b-chat                                                 NaN   \n",
       "yi-34b-chat                                                  17.17   \n",
       "claude-2.1                                                   26.26   \n",
       "claude-3-haiku-20240307                                      34.83   \n",
       "claude-3-opus-20240229                                        6.40   \n",
       "claude-3-sonnet-20240229                                     25.28   \n",
       "codegemma-7b-it                                               0.00   \n",
       "command-r-plus                                                 NaN   \n",
       "command-r                                                      NaN   \n",
       "dolphin-2.5-mixtral-8x7b                                     25.96   \n",
       "gemini-1.0-pro                                               19.97   \n",
       "gemini-1.5-flash-latest                                      20.17   \n",
       "gemini-1.5-pro-latest                                        11.58   \n",
       "gemma-1.1-2b-it                                                NaN   \n",
       "gemma-1.1-7b-it                                              10.10   \n",
       "gemma-7b-it                                                    NaN   \n",
       "gpt-3.5-turbo-0125                                           31.04   \n",
       "gpt-4-0125-preview                                            6.92   \n",
       "gpt-4-0613                                                    4.12   \n",
       "gpt-4-1106-preview                                           10.69   \n",
       "gpt-4-turbo-2024-04-09                                        9.45   \n",
       "gpt-4o-2024-05-13                                             5.56   \n",
       "llama-2-70b-chat                                               NaN   \n",
       "mistral-large-2402                                           27.28   \n",
       "mistral-medium-2312                                          24.12   \n",
       "openchat-3.5-0106                                            23.59   \n",
       "openchat-3.5-1210                                            24.23   \n",
       "openchat_3.5                                                 22.12   \n",
       "sheep-duck-llama-2-13b                                         NaN   \n",
       "sheep-duck-llama-2-70b-v1.1                                    NaN   \n",
       "tulu-2-dpo-70b                                                 NaN   \n",
       "vicuna-13b-v1.5                                               8.84   \n",
       "vicuna-33b-v1.3                                                NaN   \n",
       "\n",
       "                                referencegame, % Played  \\\n",
       "models                                                    \n",
       "codellama-34b-instruct                           100.00   \n",
       "llama-3-70b-instruct                             100.00   \n",
       "llama-3-8b-instruct                              100.00   \n",
       "mistral-7b-instruct-v0.1                         100.00   \n",
       "mistral-7b-instruct-v0.2                         100.00   \n",
       "mixtral-8x22b-instruct-v0.1                      100.00   \n",
       "mixtral-8x7b-instruct-v0.1                        61.67   \n",
       "nous-hermes-2-mixtral-8x7b-sft                    97.78   \n",
       "qwen1.5-0.5b-chat                                  0.00   \n",
       "qwen1.5-1.8b-chat                                  0.00   \n",
       "qwen1.5-14b-chat                                 100.00   \n",
       "qwen1.5-32b-chat                                 100.00   \n",
       "qwen1.5-72b-chat                                 100.00   \n",
       "qwen1.5-7b-chat                                  100.00   \n",
       "sus-chat-34b                                     100.00   \n",
       "starling-lm-7b-beta                               62.22   \n",
       "wizardlm-13b-v1.2                                100.00   \n",
       "wizardlm-70b-v1.0                                100.00   \n",
       "yi-1.5-34b-chat                                  100.00   \n",
       "yi-1.5-6b-chat                                    88.33   \n",
       "yi-1.5-9b-chat                                    51.67   \n",
       "yi-34b-chat                                        3.33   \n",
       "claude-2.1                                       100.00   \n",
       "claude-3-haiku-20240307                          100.00   \n",
       "claude-3-opus-20240229                           100.00   \n",
       "claude-3-sonnet-20240229                         100.00   \n",
       "codegemma-7b-it                                   81.67   \n",
       "command-r-plus                                    99.44   \n",
       "command-r                                        100.00   \n",
       "dolphin-2.5-mixtral-8x7b                         100.00   \n",
       "gemini-1.0-pro                                   100.00   \n",
       "gemini-1.5-flash-latest                          100.00   \n",
       "gemini-1.5-pro-latest                            100.00   \n",
       "gemma-1.1-2b-it                                  100.00   \n",
       "gemma-1.1-7b-it                                  100.00   \n",
       "gemma-7b-it                                       97.78   \n",
       "gpt-3.5-turbo-0125                               100.00   \n",
       "gpt-4-0125-preview                                99.44   \n",
       "gpt-4-0613                                       100.00   \n",
       "gpt-4-1106-preview                               100.00   \n",
       "gpt-4-turbo-2024-04-09                           100.00   \n",
       "gpt-4o-2024-05-13                                100.00   \n",
       "llama-2-70b-chat                                  46.67   \n",
       "mistral-large-2402                               100.00   \n",
       "mistral-medium-2312                               76.11   \n",
       "openchat-3.5-0106                                100.00   \n",
       "openchat-3.5-1210                                100.00   \n",
       "openchat_3.5                                     100.00   \n",
       "sheep-duck-llama-2-13b                            96.67   \n",
       "sheep-duck-llama-2-70b-v1.1                      100.00   \n",
       "tulu-2-dpo-70b                                   100.00   \n",
       "vicuna-13b-v1.5                                  100.00   \n",
       "vicuna-33b-v1.3                                  100.00   \n",
       "\n",
       "                                referencegame, Quality Score  \\\n",
       "models                                                         \n",
       "codellama-34b-instruct                                 94.44   \n",
       "llama-3-70b-instruct                                   64.44   \n",
       "llama-3-8b-instruct                                    46.11   \n",
       "mistral-7b-instruct-v0.1                               55.00   \n",
       "mistral-7b-instruct-v0.2                               38.33   \n",
       "mixtral-8x22b-instruct-v0.1                            36.67   \n",
       "mixtral-8x7b-instruct-v0.1                             41.44   \n",
       "nous-hermes-2-mixtral-8x7b-sft                         36.93   \n",
       "qwen1.5-0.5b-chat                                        NaN   \n",
       "qwen1.5-1.8b-chat                                        NaN   \n",
       "qwen1.5-14b-chat                                       44.44   \n",
       "qwen1.5-32b-chat                                       12.78   \n",
       "qwen1.5-72b-chat                                       37.22   \n",
       "qwen1.5-7b-chat                                        20.56   \n",
       "sus-chat-34b                                           70.00   \n",
       "starling-lm-7b-beta                                    30.36   \n",
       "wizardlm-13b-v1.2                                      71.11   \n",
       "wizardlm-70b-v1.0                                      81.67   \n",
       "yi-1.5-34b-chat                                        43.33   \n",
       "yi-1.5-6b-chat                                         34.59   \n",
       "yi-1.5-9b-chat                                         41.94   \n",
       "yi-34b-chat                                            33.33   \n",
       "claude-2.1                                             50.56   \n",
       "claude-3-haiku-20240307                                17.22   \n",
       "claude-3-opus-20240229                                 29.44   \n",
       "claude-3-sonnet-20240229                               27.22   \n",
       "codegemma-7b-it                                        96.60   \n",
       "command-r-plus                                         47.49   \n",
       "command-r                                              23.33   \n",
       "dolphin-2.5-mixtral-8x7b                               35.00   \n",
       "gemini-1.0-pro                                         46.11   \n",
       "gemini-1.5-flash-latest                                61.11   \n",
       "gemini-1.5-pro-latest                                  65.00   \n",
       "gemma-1.1-2b-it                                        20.00   \n",
       "gemma-1.1-7b-it                                        92.22   \n",
       "gemma-7b-it                                            40.91   \n",
       "gpt-3.5-turbo-0125                                      3.33   \n",
       "gpt-4-0125-preview                                     31.84   \n",
       "gpt-4-0613                                             35.56   \n",
       "gpt-4-1106-preview                                     29.44   \n",
       "gpt-4-turbo-2024-04-09                                 88.89   \n",
       "gpt-4o-2024-05-13                                      90.00   \n",
       "llama-2-70b-chat                                       22.62   \n",
       "mistral-large-2402                                     25.00   \n",
       "mistral-medium-2312                                    48.91   \n",
       "openchat-3.5-0106                                      93.33   \n",
       "openchat-3.5-1210                                      90.56   \n",
       "openchat_3.5                                           73.89   \n",
       "sheep-duck-llama-2-13b                                  1.72   \n",
       "sheep-duck-llama-2-70b-v1.1                            83.33   \n",
       "tulu-2-dpo-70b                                         16.67   \n",
       "vicuna-13b-v1.5                                         0.00   \n",
       "vicuna-33b-v1.3                                         0.00   \n",
       "\n",
       "                                referencegame, Quality Score (std)  \\\n",
       "models                                                               \n",
       "codellama-34b-instruct                                       22.97   \n",
       "llama-3-70b-instruct                                         48.00   \n",
       "llama-3-8b-instruct                                          49.99   \n",
       "mistral-7b-instruct-v0.1                                     49.89   \n",
       "mistral-7b-instruct-v0.2                                     48.76   \n",
       "mixtral-8x22b-instruct-v0.1                                  48.32   \n",
       "mixtral-8x7b-instruct-v0.1                                   49.49   \n",
       "nous-hermes-2-mixtral-8x7b-sft                               48.40   \n",
       "qwen1.5-0.5b-chat                                              NaN   \n",
       "qwen1.5-1.8b-chat                                              NaN   \n",
       "qwen1.5-14b-chat                                             49.83   \n",
       "qwen1.5-32b-chat                                             33.48   \n",
       "qwen1.5-72b-chat                                             48.47   \n",
       "qwen1.5-7b-chat                                              40.52   \n",
       "sus-chat-34b                                                 45.95   \n",
       "starling-lm-7b-beta                                          46.19   \n",
       "wizardlm-13b-v1.2                                            45.45   \n",
       "wizardlm-70b-v1.0                                            38.80   \n",
       "yi-1.5-34b-chat                                              49.69   \n",
       "yi-1.5-6b-chat                                               47.72   \n",
       "yi-1.5-9b-chat                                               49.61   \n",
       "yi-34b-chat                                                  51.64   \n",
       "claude-2.1                                                   50.14   \n",
       "claude-3-haiku-20240307                                      37.86   \n",
       "claude-3-opus-20240229                                       45.71   \n",
       "claude-3-sonnet-20240229                                     44.63   \n",
       "codegemma-7b-it                                              18.19   \n",
       "command-r-plus                                               50.08   \n",
       "command-r                                                    42.41   \n",
       "dolphin-2.5-mixtral-8x7b                                     47.83   \n",
       "gemini-1.0-pro                                               49.99   \n",
       "gemini-1.5-flash-latest                                      48.89   \n",
       "gemini-1.5-pro-latest                                        47.83   \n",
       "gemma-1.1-2b-it                                              40.11   \n",
       "gemma-1.1-7b-it                                              26.86   \n",
       "gemma-7b-it                                                  49.31   \n",
       "gpt-3.5-turbo-0125                                           18.00   \n",
       "gpt-4-0125-preview                                           46.72   \n",
       "gpt-4-0613                                                   48.00   \n",
       "gpt-4-1106-preview                                           45.71   \n",
       "gpt-4-turbo-2024-04-09                                       31.51   \n",
       "gpt-4o-2024-05-13                                            30.08   \n",
       "llama-2-70b-chat                                             42.09   \n",
       "mistral-large-2402                                           43.42   \n",
       "mistral-medium-2312                                          50.17   \n",
       "openchat-3.5-0106                                            25.01   \n",
       "openchat-3.5-1210                                            29.33   \n",
       "openchat_3.5                                                 44.05   \n",
       "sheep-duck-llama-2-13b                                       13.05   \n",
       "sheep-duck-llama-2-70b-v1.1                                  37.37   \n",
       "tulu-2-dpo-70b                                               37.37   \n",
       "vicuna-13b-v1.5                                               0.00   \n",
       "vicuna-33b-v1.3                                               0.00   \n",
       "\n",
       "                                taboo, % Played  taboo, Quality Score  \\\n",
       "models                                                                  \n",
       "codellama-34b-instruct                    51.67                 51.61   \n",
       "llama-3-70b-instruct                      91.67                 70.30   \n",
       "llama-3-8b-instruct                      100.00                 37.78   \n",
       "mistral-7b-instruct-v0.1                 100.00                 31.67   \n",
       "mistral-7b-instruct-v0.2                  65.00                  0.00   \n",
       "mixtral-8x22b-instruct-v0.1               58.33                 40.00   \n",
       "mixtral-8x7b-instruct-v0.1                51.67                  9.68   \n",
       "nous-hermes-2-mixtral-8x7b-sft            93.33                 47.92   \n",
       "qwen1.5-0.5b-chat                         86.67                  1.92   \n",
       "qwen1.5-1.8b-chat                         93.33                  0.00   \n",
       "qwen1.5-14b-chat                          46.67                 41.07   \n",
       "qwen1.5-32b-chat                          61.67                 42.79   \n",
       "qwen1.5-72b-chat                          73.33                 73.11   \n",
       "qwen1.5-7b-chat                           98.33                 13.56   \n",
       "sus-chat-34b                              98.33                 52.26   \n",
       "starling-lm-7b-beta                       46.67                  0.00   \n",
       "wizardlm-13b-v1.2                         35.00                 64.29   \n",
       "wizardlm-70b-v1.0                         56.67                 70.59   \n",
       "yi-1.5-34b-chat                           66.67                  0.00   \n",
       "yi-1.5-6b-chat                            65.00                  0.00   \n",
       "yi-1.5-9b-chat                            41.67                  0.00   \n",
       "yi-34b-chat                               68.33                 41.46   \n",
       "claude-2.1                                95.00                 64.91   \n",
       "claude-3-haiku-20240307                   63.33                 78.95   \n",
       "claude-3-opus-20240229                    88.33                 83.65   \n",
       "claude-3-sonnet-20240229                 100.00                 73.61   \n",
       "codegemma-7b-it                           83.33                 26.00   \n",
       "command-r-plus                            63.33                 67.11   \n",
       "command-r                                 63.33                 44.74   \n",
       "dolphin-2.5-mixtral-8x7b                 100.00                 41.11   \n",
       "gemini-1.0-pro                            85.00                 55.23   \n",
       "gemini-1.5-flash-latest                   91.67                 57.88   \n",
       "gemini-1.5-pro-latest                     85.00                 70.59   \n",
       "gemma-1.1-2b-it                           45.00                 14.81   \n",
       "gemma-1.1-7b-it                           35.00                 52.38   \n",
       "gemma-7b-it                                0.00                   NaN   \n",
       "gpt-3.5-turbo-0125                        68.33                 73.17   \n",
       "gpt-4-0125-preview                        75.00                 93.33   \n",
       "gpt-4-0613                                86.67                 79.81   \n",
       "gpt-4-1106-preview                        91.67                 83.94   \n",
       "gpt-4-turbo-2024-04-09                    85.00                 82.35   \n",
       "gpt-4o-2024-05-13                        100.00                 75.28   \n",
       "llama-2-70b-chat                           0.00                   NaN   \n",
       "mistral-large-2402                        60.00                 88.89   \n",
       "mistral-medium-2312                       30.00                 88.89   \n",
       "openchat-3.5-0106                         65.00                 64.10   \n",
       "openchat-3.5-1210                         40.00                 66.67   \n",
       "openchat_3.5                             100.00                 45.00   \n",
       "sheep-duck-llama-2-13b                    83.33                  4.00   \n",
       "sheep-duck-llama-2-70b-v1.1               55.00                 90.91   \n",
       "tulu-2-dpo-70b                            68.33                 68.29   \n",
       "vicuna-13b-v1.5                           46.67                 60.71   \n",
       "vicuna-33b-v1.3                           46.67                 89.29   \n",
       "\n",
       "                                taboo, Quality Score (std)  wordle, % Played  \\\n",
       "models                                                                         \n",
       "codellama-34b-instruct                               50.80             56.67   \n",
       "llama-3-70b-instruct                                 39.37             90.00   \n",
       "llama-3-8b-instruct                                  45.08             86.67   \n",
       "mistral-7b-instruct-v0.1                             45.07              0.00   \n",
       "mistral-7b-instruct-v0.2                              0.00             50.00   \n",
       "mixtral-8x22b-instruct-v0.1                          49.71             96.67   \n",
       "mixtral-8x7b-instruct-v0.1                           30.05             96.67   \n",
       "nous-hermes-2-mixtral-8x7b-sft                       47.36              0.00   \n",
       "qwen1.5-0.5b-chat                                    13.87             46.67   \n",
       "qwen1.5-1.8b-chat                                     0.00              0.00   \n",
       "qwen1.5-14b-chat                                     47.25             90.00   \n",
       "qwen1.5-32b-chat                                     47.39             93.33   \n",
       "qwen1.5-72b-chat                                     43.02             96.67   \n",
       "qwen1.5-7b-chat                                      33.26              0.00   \n",
       "sus-chat-34b                                         45.64             93.33   \n",
       "starling-lm-7b-beta                                   0.00             66.67   \n",
       "wizardlm-13b-v1.2                                    45.12             26.67   \n",
       "wizardlm-70b-v1.0                                    44.58             73.33   \n",
       "yi-1.5-34b-chat                                       0.00             96.67   \n",
       "yi-1.5-6b-chat                                        0.00             86.67   \n",
       "yi-1.5-9b-chat                                        0.00             86.67   \n",
       "yi-34b-chat                                          49.88             83.33   \n",
       "claude-2.1                                           45.93             96.67   \n",
       "claude-3-haiku-20240307                              32.11            100.00   \n",
       "claude-3-opus-20240229                               32.11            100.00   \n",
       "claude-3-sonnet-20240229                             36.73            100.00   \n",
       "codegemma-7b-it                                      44.31             96.67   \n",
       "command-r-plus                                       45.44            100.00   \n",
       "command-r                                            47.63             93.33   \n",
       "dolphin-2.5-mixtral-8x7b                             46.79              0.00   \n",
       "gemini-1.0-pro                                       44.53             90.00   \n",
       "gemini-1.5-flash-latest                              43.61             96.67   \n",
       "gemini-1.5-pro-latest                                35.84            100.00   \n",
       "gemma-1.1-2b-it                                      36.20              0.00   \n",
       "gemma-1.1-7b-it                                      51.18             73.33   \n",
       "gemma-7b-it                                            NaN              3.33   \n",
       "gpt-3.5-turbo-0125                                   41.98            100.00   \n",
       "gpt-4-0125-preview                                   20.23            100.00   \n",
       "gpt-4-0613                                           33.22            100.00   \n",
       "gpt-4-1106-preview                                   29.57            100.00   \n",
       "gpt-4-turbo-2024-04-09                               30.81            100.00   \n",
       "gpt-4o-2024-05-13                                    35.19            100.00   \n",
       "llama-2-70b-chat                                       NaN              0.00   \n",
       "mistral-large-2402                                   31.87             63.33   \n",
       "mistral-medium-2312                                  32.34             80.00   \n",
       "openchat-3.5-0106                                    48.60             10.00   \n",
       "openchat-3.5-1210                                    48.15             46.67   \n",
       "openchat_3.5                                         49.32             90.00   \n",
       "sheep-duck-llama-2-13b                               19.79              0.00   \n",
       "sheep-duck-llama-2-70b-v1.1                          29.19             60.00   \n",
       "tulu-2-dpo-70b                                       47.11             80.00   \n",
       "vicuna-13b-v1.5                                      49.73             53.33   \n",
       "vicuna-33b-v1.3                                      31.50              0.00   \n",
       "\n",
       "                                wordle, Quality Score  \\\n",
       "models                                                  \n",
       "codellama-34b-instruct                           0.00   \n",
       "llama-3-70b-instruct                             1.85   \n",
       "llama-3-8b-instruct                              0.00   \n",
       "mistral-7b-instruct-v0.1                          NaN   \n",
       "mistral-7b-instruct-v0.2                         0.00   \n",
       "mixtral-8x22b-instruct-v0.1                      0.00   \n",
       "mixtral-8x7b-instruct-v0.1                       0.00   \n",
       "nous-hermes-2-mixtral-8x7b-sft                    NaN   \n",
       "qwen1.5-0.5b-chat                                0.00   \n",
       "qwen1.5-1.8b-chat                                 NaN   \n",
       "qwen1.5-14b-chat                                 0.00   \n",
       "qwen1.5-32b-chat                                 0.00   \n",
       "qwen1.5-72b-chat                                 0.69   \n",
       "qwen1.5-7b-chat                                   NaN   \n",
       "sus-chat-34b                                     0.00   \n",
       "starling-lm-7b-beta                              0.00   \n",
       "wizardlm-13b-v1.2                                0.00   \n",
       "wizardlm-70b-v1.0                                0.00   \n",
       "yi-1.5-34b-chat                                  0.00   \n",
       "yi-1.5-6b-chat                                   0.00   \n",
       "yi-1.5-9b-chat                                   0.00   \n",
       "yi-34b-chat                                      0.00   \n",
       "claude-2.1                                       7.59   \n",
       "claude-3-haiku-20240307                          0.00   \n",
       "claude-3-opus-20240229                          20.00   \n",
       "claude-3-sonnet-20240229                        10.67   \n",
       "codegemma-7b-it                                  0.00   \n",
       "command-r-plus                                   7.33   \n",
       "command-r                                        0.00   \n",
       "dolphin-2.5-mixtral-8x7b                          NaN   \n",
       "gemini-1.0-pro                                   0.74   \n",
       "gemini-1.5-flash-latest                          0.69   \n",
       "gemini-1.5-pro-latest                           10.67   \n",
       "gemma-1.1-2b-it                                   NaN   \n",
       "gemma-1.1-7b-it                                  0.00   \n",
       "gemma-7b-it                                      0.00   \n",
       "gpt-3.5-turbo-0125                               0.00   \n",
       "gpt-4-0125-preview                              20.67   \n",
       "gpt-4-0613                                       9.00   \n",
       "gpt-4-1106-preview                              13.00   \n",
       "gpt-4-turbo-2024-04-09                          16.33   \n",
       "gpt-4o-2024-05-13                               19.33   \n",
       "llama-2-70b-chat                                  NaN   \n",
       "mistral-large-2402                               5.26   \n",
       "mistral-medium-2312                              0.00   \n",
       "openchat-3.5-0106                                0.00   \n",
       "openchat-3.5-1210                                0.00   \n",
       "openchat_3.5                                     0.00   \n",
       "sheep-duck-llama-2-13b                            NaN   \n",
       "sheep-duck-llama-2-70b-v1.1                      0.00   \n",
       "tulu-2-dpo-70b                                   0.00   \n",
       "vicuna-13b-v1.5                                  0.00   \n",
       "vicuna-33b-v1.3                                   NaN   \n",
       "\n",
       "                                wordle, Quality Score (std)  \\\n",
       "models                                                        \n",
       "codellama-34b-instruct                                 0.00   \n",
       "llama-3-70b-instruct                                   6.81   \n",
       "llama-3-8b-instruct                                    0.00   \n",
       "mistral-7b-instruct-v0.1                                NaN   \n",
       "mistral-7b-instruct-v0.2                               0.00   \n",
       "mixtral-8x22b-instruct-v0.1                            0.00   \n",
       "mixtral-8x7b-instruct-v0.1                             0.00   \n",
       "nous-hermes-2-mixtral-8x7b-sft                          NaN   \n",
       "qwen1.5-0.5b-chat                                      0.00   \n",
       "qwen1.5-1.8b-chat                                       NaN   \n",
       "qwen1.5-14b-chat                                       0.00   \n",
       "qwen1.5-32b-chat                                       0.00   \n",
       "qwen1.5-72b-chat                                       3.71   \n",
       "qwen1.5-7b-chat                                         NaN   \n",
       "sus-chat-34b                                           0.00   \n",
       "starling-lm-7b-beta                                    0.00   \n",
       "wizardlm-13b-v1.2                                      0.00   \n",
       "wizardlm-70b-v1.0                                      0.00   \n",
       "yi-1.5-34b-chat                                        0.00   \n",
       "yi-1.5-6b-chat                                         0.00   \n",
       "yi-1.5-9b-chat                                         0.00   \n",
       "yi-34b-chat                                            0.00   \n",
       "claude-2.1                                            21.16   \n",
       "claude-3-haiku-20240307                                0.00   \n",
       "claude-3-opus-20240229                                28.65   \n",
       "claude-3-sonnet-20240229                              23.33   \n",
       "codegemma-7b-it                                        0.00   \n",
       "command-r-plus                                        19.82   \n",
       "command-r                                              0.00   \n",
       "dolphin-2.5-mixtral-8x7b                                NaN   \n",
       "gemini-1.0-pro                                         3.85   \n",
       "gemini-1.5-flash-latest                                3.71   \n",
       "gemini-1.5-pro-latest                                 22.43   \n",
       "gemma-1.1-2b-it                                         NaN   \n",
       "gemma-1.1-7b-it                                        0.00   \n",
       "gemma-7b-it                                             NaN   \n",
       "gpt-3.5-turbo-0125                                     0.00   \n",
       "gpt-4-0125-preview                                    27.66   \n",
       "gpt-4-0613                                            25.78   \n",
       "gpt-4-1106-preview                                    27.56   \n",
       "gpt-4-turbo-2024-04-09                                31.35   \n",
       "gpt-4o-2024-05-13                                     28.52   \n",
       "llama-2-70b-chat                                        NaN   \n",
       "mistral-large-2402                                    22.94   \n",
       "mistral-medium-2312                                    0.00   \n",
       "openchat-3.5-0106                                      0.00   \n",
       "openchat-3.5-1210                                      0.00   \n",
       "openchat_3.5                                           0.00   \n",
       "sheep-duck-llama-2-13b                                  NaN   \n",
       "sheep-duck-llama-2-70b-v1.1                            0.00   \n",
       "tulu-2-dpo-70b                                         0.00   \n",
       "vicuna-13b-v1.5                                        0.00   \n",
       "vicuna-33b-v1.3                                         NaN   \n",
       "\n",
       "                                wordle_withclue, % Played  \\\n",
       "models                                                      \n",
       "codellama-34b-instruct                              26.67   \n",
       "llama-3-70b-instruct                                96.67   \n",
       "llama-3-8b-instruct                                 83.33   \n",
       "mistral-7b-instruct-v0.1                            23.33   \n",
       "mistral-7b-instruct-v0.2                            26.67   \n",
       "mixtral-8x22b-instruct-v0.1                         60.00   \n",
       "mixtral-8x7b-instruct-v0.1                          76.67   \n",
       "nous-hermes-2-mixtral-8x7b-sft                      53.33   \n",
       "qwen1.5-0.5b-chat                                   40.00   \n",
       "qwen1.5-1.8b-chat                                   10.00   \n",
       "qwen1.5-14b-chat                                    16.67   \n",
       "qwen1.5-32b-chat                                    60.00   \n",
       "qwen1.5-72b-chat                                    90.00   \n",
       "qwen1.5-7b-chat                                     10.00   \n",
       "sus-chat-34b                                        43.33   \n",
       "starling-lm-7b-beta                                 33.33   \n",
       "wizardlm-13b-v1.2                                   53.33   \n",
       "wizardlm-70b-v1.0                                   56.67   \n",
       "yi-1.5-34b-chat                                     70.00   \n",
       "yi-1.5-6b-chat                                      33.33   \n",
       "yi-1.5-9b-chat                                      46.67   \n",
       "yi-34b-chat                                         43.33   \n",
       "claude-2.1                                          86.67   \n",
       "claude-3-haiku-20240307                            100.00   \n",
       "claude-3-opus-20240229                              96.67   \n",
       "claude-3-sonnet-20240229                           100.00   \n",
       "codegemma-7b-it                                     43.33   \n",
       "command-r-plus                                      93.33   \n",
       "command-r                                           66.67   \n",
       "dolphin-2.5-mixtral-8x7b                            43.33   \n",
       "gemini-1.0-pro                                      86.67   \n",
       "gemini-1.5-flash-latest                             66.67   \n",
       "gemini-1.5-pro-latest                               93.33   \n",
       "gemma-1.1-2b-it                                      6.67   \n",
       "gemma-1.1-7b-it                                     76.67   \n",
       "gemma-7b-it                                          3.33   \n",
       "gpt-3.5-turbo-0125                                  96.67   \n",
       "gpt-4-0125-preview                                 100.00   \n",
       "gpt-4-0613                                         100.00   \n",
       "gpt-4-1106-preview                                 100.00   \n",
       "gpt-4-turbo-2024-04-09                             100.00   \n",
       "gpt-4o-2024-05-13                                  100.00   \n",
       "llama-2-70b-chat                                     3.33   \n",
       "mistral-large-2402                                  83.33   \n",
       "mistral-medium-2312                                 83.33   \n",
       "openchat-3.5-0106                                   40.00   \n",
       "openchat-3.5-1210                                   53.33   \n",
       "openchat_3.5                                        36.67   \n",
       "sheep-duck-llama-2-13b                              23.33   \n",
       "sheep-duck-llama-2-70b-v1.1                         43.33   \n",
       "tulu-2-dpo-70b                                      53.33   \n",
       "vicuna-13b-v1.5                                     36.67   \n",
       "vicuna-33b-v1.3                                     10.00   \n",
       "\n",
       "                                wordle_withclue, Quality Score  \\\n",
       "models                                                           \n",
       "codellama-34b-instruct                                   25.00   \n",
       "llama-3-70b-instruct                                     14.37   \n",
       "llama-3-8b-instruct                                      14.00   \n",
       "mistral-7b-instruct-v0.1                                  0.00   \n",
       "mistral-7b-instruct-v0.2                                 43.75   \n",
       "mixtral-8x22b-instruct-v0.1                              15.00   \n",
       "mixtral-8x7b-instruct-v0.1                               19.13   \n",
       "nous-hermes-2-mixtral-8x7b-sft                           15.62   \n",
       "qwen1.5-0.5b-chat                                         0.00   \n",
       "qwen1.5-1.8b-chat                                         0.00   \n",
       "qwen1.5-14b-chat                                         40.00   \n",
       "qwen1.5-32b-chat                                         16.85   \n",
       "qwen1.5-72b-chat                                         20.93   \n",
       "qwen1.5-7b-chat                                           0.00   \n",
       "sus-chat-34b                                             23.08   \n",
       "starling-lm-7b-beta                                       0.00   \n",
       "wizardlm-13b-v1.2                                         6.25   \n",
       "wizardlm-70b-v1.0                                        17.84   \n",
       "yi-1.5-34b-chat                                          18.25   \n",
       "yi-1.5-6b-chat                                           20.00   \n",
       "yi-1.5-9b-chat                                            7.14   \n",
       "yi-34b-chat                                              26.92   \n",
       "claude-2.1                                               21.60   \n",
       "claude-3-haiku-20240307                                   8.44   \n",
       "claude-3-opus-20240229                                   46.09   \n",
       "claude-3-sonnet-20240229                                 20.50   \n",
       "codegemma-7b-it                                          14.10   \n",
       "command-r-plus                                           26.79   \n",
       "command-r                                                30.00   \n",
       "dolphin-2.5-mixtral-8x7b                                  7.69   \n",
       "gemini-1.0-pro                                           12.82   \n",
       "gemini-1.5-flash-latest                                  33.33   \n",
       "gemini-1.5-pro-latest                                    41.37   \n",
       "gemma-1.1-2b-it                                           0.00   \n",
       "gemma-1.1-7b-it                                           6.52   \n",
       "gemma-7b-it                                               0.00   \n",
       "gpt-3.5-turbo-0125                                       24.25   \n",
       "gpt-4-0125-preview                                       33.17   \n",
       "gpt-4-0613                                               36.78   \n",
       "gpt-4-1106-preview                                       29.00   \n",
       "gpt-4-turbo-2024-04-09                                   29.89   \n",
       "gpt-4o-2024-05-13                                        28.00   \n",
       "llama-2-70b-chat                                          0.00   \n",
       "mistral-large-2402                                       26.80   \n",
       "mistral-medium-2312                                      26.80   \n",
       "openchat-3.5-0106                                        12.50   \n",
       "openchat-3.5-1210                                        20.83   \n",
       "openchat_3.5                                             15.15   \n",
       "sheep-duck-llama-2-13b                                   28.57   \n",
       "sheep-duck-llama-2-70b-v1.1                              42.31   \n",
       "tulu-2-dpo-70b                                           16.88   \n",
       "vicuna-13b-v1.5                                          21.21   \n",
       "vicuna-33b-v1.3                                          16.67   \n",
       "\n",
       "                                wordle_withclue, Quality Score (std)  \\\n",
       "models                                                                 \n",
       "codellama-34b-instruct                                         46.29   \n",
       "llama-3-70b-instruct                                           32.34   \n",
       "llama-3-8b-instruct                                            33.91   \n",
       "mistral-7b-instruct-v0.1                                        0.00   \n",
       "mistral-7b-instruct-v0.2                                       49.55   \n",
       "mixtral-8x22b-instruct-v0.1                                    33.30   \n",
       "mixtral-8x7b-instruct-v0.1                                     35.28   \n",
       "nous-hermes-2-mixtral-8x7b-sft                                 30.10   \n",
       "qwen1.5-0.5b-chat                                               0.00   \n",
       "qwen1.5-1.8b-chat                                               0.00   \n",
       "qwen1.5-14b-chat                                               54.77   \n",
       "qwen1.5-32b-chat                                               33.34   \n",
       "qwen1.5-72b-chat                                               39.03   \n",
       "qwen1.5-7b-chat                                                 0.00   \n",
       "sus-chat-34b                                                   43.85   \n",
       "starling-lm-7b-beta                                             0.00   \n",
       "wizardlm-13b-v1.2                                              25.00   \n",
       "wizardlm-70b-v1.0                                              34.09   \n",
       "yi-1.5-34b-chat                                                36.48   \n",
       "yi-1.5-6b-chat                                                 42.16   \n",
       "yi-1.5-9b-chat                                                 26.73   \n",
       "yi-34b-chat                                                    43.85   \n",
       "claude-2.1                                                     39.58   \n",
       "claude-3-haiku-20240307                                        21.27   \n",
       "claude-3-opus-20240229                                         38.59   \n",
       "claude-3-sonnet-20240229                                       33.65   \n",
       "codegemma-7b-it                                                30.31   \n",
       "command-r-plus                                                 37.91   \n",
       "command-r                                                      44.13   \n",
       "dolphin-2.5-mixtral-8x7b                                       27.74   \n",
       "gemini-1.0-pro                                                 32.76   \n",
       "gemini-1.5-flash-latest                                        38.90   \n",
       "gemini-1.5-pro-latest                                          39.25   \n",
       "gemma-1.1-2b-it                                                 0.00   \n",
       "gemma-1.1-7b-it                                                22.88   \n",
       "gemma-7b-it                                                      NaN   \n",
       "gpt-3.5-turbo-0125                                             40.95   \n",
       "gpt-4-0125-preview                                             42.87   \n",
       "gpt-4-0613                                                     40.40   \n",
       "gpt-4-1106-preview                                             39.53   \n",
       "gpt-4-turbo-2024-04-09                                         39.18   \n",
       "gpt-4o-2024-05-13                                              36.38   \n",
       "llama-2-70b-chat                                                 NaN   \n",
       "mistral-large-2402                                             36.21   \n",
       "mistral-medium-2312                                            42.67   \n",
       "openchat-3.5-0106                                              31.08   \n",
       "openchat-3.5-1210                                              40.14   \n",
       "openchat_3.5                                                   31.14   \n",
       "sheep-duck-llama-2-13b                                         48.80   \n",
       "sheep-duck-llama-2-70b-v1.1                                    44.94   \n",
       "tulu-2-dpo-70b                                                 29.83   \n",
       "vicuna-13b-v1.5                                                40.20   \n",
       "vicuna-33b-v1.3                                                28.87   \n",
       "\n",
       "                                wordle_withcritic, % Played  \\\n",
       "models                                                        \n",
       "codellama-34b-instruct                                 0.00   \n",
       "llama-3-70b-instruct                                  86.67   \n",
       "llama-3-8b-instruct                                   66.67   \n",
       "mistral-7b-instruct-v0.1                              16.67   \n",
       "mistral-7b-instruct-v0.2                              16.67   \n",
       "mixtral-8x22b-instruct-v0.1                           50.00   \n",
       "mixtral-8x7b-instruct-v0.1                            46.67   \n",
       "nous-hermes-2-mixtral-8x7b-sft                        33.33   \n",
       "qwen1.5-0.5b-chat                                      6.67   \n",
       "qwen1.5-1.8b-chat                                      3.33   \n",
       "qwen1.5-14b-chat                                       3.33   \n",
       "qwen1.5-32b-chat                                      43.33   \n",
       "qwen1.5-72b-chat                                      43.33   \n",
       "qwen1.5-7b-chat                                        3.33   \n",
       "sus-chat-34b                                          23.33   \n",
       "starling-lm-7b-beta                                    3.33   \n",
       "wizardlm-13b-v1.2                                     20.00   \n",
       "wizardlm-70b-v1.0                                     36.67   \n",
       "yi-1.5-34b-chat                                       33.33   \n",
       "yi-1.5-6b-chat                                        16.67   \n",
       "yi-1.5-9b-chat                                        40.00   \n",
       "yi-34b-chat                                           26.67   \n",
       "claude-2.1                                            96.67   \n",
       "claude-3-haiku-20240307                               93.33   \n",
       "claude-3-opus-20240229                                96.67   \n",
       "claude-3-sonnet-20240229                              96.67   \n",
       "codegemma-7b-it                                       16.67   \n",
       "command-r-plus                                        93.33   \n",
       "command-r                                             46.67   \n",
       "dolphin-2.5-mixtral-8x7b                              33.33   \n",
       "gemini-1.0-pro                                        93.33   \n",
       "gemini-1.5-flash-latest                               80.00   \n",
       "gemini-1.5-pro-latest                                 96.67   \n",
       "gemma-1.1-2b-it                                        6.67   \n",
       "gemma-1.1-7b-it                                       56.67   \n",
       "gemma-7b-it                                           20.00   \n",
       "gpt-3.5-turbo-0125                                    96.67   \n",
       "gpt-4-0125-preview                                    90.00   \n",
       "gpt-4-0613                                           100.00   \n",
       "gpt-4-1106-preview                                   100.00   \n",
       "gpt-4-turbo-2024-04-09                                96.67   \n",
       "gpt-4o-2024-05-13                                    100.00   \n",
       "llama-2-70b-chat                                       0.00   \n",
       "mistral-large-2402                                    63.33   \n",
       "mistral-medium-2312                                   53.33   \n",
       "openchat-3.5-0106                                     20.00   \n",
       "openchat-3.5-1210                                     23.33   \n",
       "openchat_3.5                                          30.00   \n",
       "sheep-duck-llama-2-13b                                20.00   \n",
       "sheep-duck-llama-2-70b-v1.1                           30.00   \n",
       "tulu-2-dpo-70b                                        46.67   \n",
       "vicuna-13b-v1.5                                       20.00   \n",
       "vicuna-33b-v1.3                                       10.00   \n",
       "\n",
       "                                wordle_withcritic, Quality Score  \\\n",
       "models                                                             \n",
       "codellama-34b-instruct                                       NaN   \n",
       "llama-3-70b-instruct                                       25.64   \n",
       "llama-3-8b-instruct                                         0.83   \n",
       "mistral-7b-instruct-v0.1                                   20.00   \n",
       "mistral-7b-instruct-v0.2                                   50.00   \n",
       "mixtral-8x22b-instruct-v0.1                                30.00   \n",
       "mixtral-8x7b-instruct-v0.1                                 15.48   \n",
       "nous-hermes-2-mixtral-8x7b-sft                             20.00   \n",
       "qwen1.5-0.5b-chat                                           0.00   \n",
       "qwen1.5-1.8b-chat                                           0.00   \n",
       "qwen1.5-14b-chat                                          100.00   \n",
       "qwen1.5-32b-chat                                           19.23   \n",
       "qwen1.5-72b-chat                                           30.77   \n",
       "qwen1.5-7b-chat                                             0.00   \n",
       "sus-chat-34b                                                7.14   \n",
       "starling-lm-7b-beta                                         0.00   \n",
       "wizardlm-13b-v1.2                                          22.22   \n",
       "wizardlm-70b-v1.0                                          18.18   \n",
       "yi-1.5-34b-chat                                            11.67   \n",
       "yi-1.5-6b-chat                                             26.67   \n",
       "yi-1.5-9b-chat                                              8.33   \n",
       "yi-34b-chat                                                22.92   \n",
       "claude-2.1                                                 17.82   \n",
       "claude-3-haiku-20240307                                    14.58   \n",
       "claude-3-opus-20240229                                     31.78   \n",
       "claude-3-sonnet-20240229                                   22.13   \n",
       "codegemma-7b-it                                            40.00   \n",
       "command-r-plus                                             17.80   \n",
       "command-r                                                  16.67   \n",
       "dolphin-2.5-mixtral-8x7b                                   20.00   \n",
       "gemini-1.0-pro                                              7.74   \n",
       "gemini-1.5-flash-latest                                    20.97   \n",
       "gemini-1.5-pro-latest                                      32.99   \n",
       "gemma-1.1-2b-it                                            16.66   \n",
       "gemma-1.1-7b-it                                             8.82   \n",
       "gemma-7b-it                                                 0.00   \n",
       "gpt-3.5-turbo-0125                                         10.92   \n",
       "gpt-4-0125-preview                                         18.33   \n",
       "gpt-4-0613                                                 20.28   \n",
       "gpt-4-1106-preview                                         34.22   \n",
       "gpt-4-turbo-2024-04-09                                     20.23   \n",
       "gpt-4o-2024-05-13                                          31.11   \n",
       "llama-2-70b-chat                                             NaN   \n",
       "mistral-large-2402                                         29.82   \n",
       "mistral-medium-2312                                        20.31   \n",
       "openchat-3.5-0106                                           0.00   \n",
       "openchat-3.5-1210                                           7.14   \n",
       "openchat_3.5                                               44.44   \n",
       "sheep-duck-llama-2-13b                                     33.33   \n",
       "sheep-duck-llama-2-70b-v1.1                                44.44   \n",
       "tulu-2-dpo-70b                                             25.00   \n",
       "vicuna-13b-v1.5                                             4.17   \n",
       "vicuna-33b-v1.3                                            83.33   \n",
       "\n",
       "                                wordle_withcritic, Quality Score (std) ow/gt  \n",
       "models                                                                        \n",
       "codellama-34b-instruct                                             NaN    ow  \n",
       "llama-3-70b-instruct                                             39.08    ow  \n",
       "llama-3-8b-instruct                                               3.73    ow  \n",
       "mistral-7b-instruct-v0.1                                         44.72    ow  \n",
       "mistral-7b-instruct-v0.2                                         50.00    ow  \n",
       "mixtral-8x22b-instruct-v0.1                                      41.40    ow  \n",
       "mixtral-8x7b-instruct-v0.1                                       36.08    ow  \n",
       "nous-hermes-2-mixtral-8x7b-sft                                   42.16    ow  \n",
       "qwen1.5-0.5b-chat                                                 0.00    ow  \n",
       "qwen1.5-1.8b-chat                                                  NaN    ow  \n",
       "qwen1.5-14b-chat                                                   NaN    ow  \n",
       "qwen1.5-32b-chat                                                 38.40    ow  \n",
       "qwen1.5-72b-chat                                                 48.04    ow  \n",
       "qwen1.5-7b-chat                                                    NaN    ow  \n",
       "sus-chat-34b                                                     18.90    ow  \n",
       "starling-lm-7b-beta                                                NaN    ow  \n",
       "wizardlm-13b-v1.2                                                40.37    ow  \n",
       "wizardlm-70b-v1.0                                                40.45    ow  \n",
       "yi-1.5-34b-chat                                                  31.48    ow  \n",
       "yi-1.5-6b-chat                                                   43.46    ow  \n",
       "yi-1.5-9b-chat                                                   28.87    ow  \n",
       "yi-34b-chat                                                      36.66    ow  \n",
       "claude-2.1                                                       35.34    $$  \n",
       "claude-3-haiku-20240307                                          31.64    $$  \n",
       "claude-3-opus-20240229                                           35.15    $$  \n",
       "claude-3-sonnet-20240229                                         33.35    $$  \n",
       "codegemma-7b-it                                                  54.77    ow  \n",
       "command-r-plus                                                   32.58    ow  \n",
       "command-r                                                        36.40    ow  \n",
       "dolphin-2.5-mixtral-8x7b                                         42.16    ow  \n",
       "gemini-1.0-pro                                                   21.98    $$  \n",
       "gemini-1.5-flash-latest                                          31.07    $$  \n",
       "gemini-1.5-pro-latest                                            35.32    $$  \n",
       "gemma-1.1-2b-it                                                  23.57    ow  \n",
       "gemma-1.1-7b-it                                                  26.43    ow  \n",
       "gemma-7b-it                                                       0.00    ow  \n",
       "gpt-3.5-turbo-0125                                               27.56    $$  \n",
       "gpt-4-0125-preview                                               32.69    $$  \n",
       "gpt-4-0613                                                       29.17    $$  \n",
       "gpt-4-1106-preview                                               39.55    $$  \n",
       "gpt-4-turbo-2024-04-09                                           28.21    $$  \n",
       "gpt-4o-2024-05-13                                                33.14    $$  \n",
       "llama-2-70b-chat                                                   NaN    ow  \n",
       "mistral-large-2402                                               40.57    $$  \n",
       "mistral-medium-2312                                              40.02    $$  \n",
       "openchat-3.5-0106                                                 0.00    ow  \n",
       "openchat-3.5-1210                                                18.90    ow  \n",
       "openchat_3.5                                                     52.70    ow  \n",
       "sheep-duck-llama-2-13b                                           51.64    ow  \n",
       "sheep-duck-llama-2-70b-v1.1                                      46.40    ow  \n",
       "tulu-2-dpo-70b                                                   42.74    ow  \n",
       "vicuna-13b-v1.5                                                  10.21    ow  \n",
       "vicuna-33b-v1.3                                                  28.87    ow  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0da38b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-, clemscore</th>\n",
       "      <th>ow/gt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo-2024-04-09</th>\n",
       "      <td>58.30</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0125-preview</th>\n",
       "      <td>52.50</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-1106-preview</th>\n",
       "      <td>51.99</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0613</th>\n",
       "      <td>51.09</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o-2024-05-13</th>\n",
       "      <td>48.34</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-opus-20240229</th>\n",
       "      <td>42.42</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-1.5-pro-latest</th>\n",
       "      <td>41.72</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-3-70b-instruct</th>\n",
       "      <td>35.11</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-2.1</th>\n",
       "      <td>32.50</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-1.5-flash-latest</th>\n",
       "      <td>32.00</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-sonnet-20240229</th>\n",
       "      <td>30.53</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-72b-chat</th>\n",
       "      <td>30.37</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-large-2402</th>\n",
       "      <td>28.17</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0125</th>\n",
       "      <td>27.22</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-1.0-pro</th>\n",
       "      <td>26.95</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>command-r-plus</th>\n",
       "      <td>24.94</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat_3.5</th>\n",
       "      <td>23.64</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-haiku-20240307</th>\n",
       "      <td>22.49</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sheep-duck-llama-2-70b-v1.1</th>\n",
       "      <td>21.50</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-3-8b-instruct</th>\n",
       "      <td>19.99</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat-3.5-1210</th>\n",
       "      <td>18.22</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-70b-v1.0</th>\n",
       "      <td>17.40</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat-3.5-0106</th>\n",
       "      <td>17.10</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-14b-chat</th>\n",
       "      <td>16.80</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-medium-2312</th>\n",
       "      <td>16.43</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-32b-chat</th>\n",
       "      <td>15.41</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegemma-7b-it</th>\n",
       "      <td>15.30</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dolphin-2.5-mixtral-8x7b</th>\n",
       "      <td>15.10</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codellama-34b-instruct</th>\n",
       "      <td>14.35</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>command-r</th>\n",
       "      <td>14.15</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-1.1-7b-it</th>\n",
       "      <td>14.14</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sus-chat-34b</th>\n",
       "      <td>14.11</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixtral-8x22b-instruct-v0.1</th>\n",
       "      <td>12.69</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tulu-2-dpo-70b</th>\n",
       "      <td>12.62</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nous-hermes-2-mixtral-8x7b-sft</th>\n",
       "      <td>11.95</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-13b-v1.2</th>\n",
       "      <td>11.48</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-33b-v1.3</th>\n",
       "      <td>11.27</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-instruct-v0.2</th>\n",
       "      <td>9.75</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-34b-chat</th>\n",
       "      <td>8.27</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixtral-8x7b-instruct-v0.1</th>\n",
       "      <td>8.17</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-instruct-v0.1</th>\n",
       "      <td>8.01</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-1.5-34b-chat</th>\n",
       "      <td>7.67</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.5</th>\n",
       "      <td>7.01</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-1.5-6b-chat</th>\n",
       "      <td>6.73</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starling-lm-7b-beta</th>\n",
       "      <td>6.56</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sheep-duck-llama-2-13b</th>\n",
       "      <td>5.39</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-1.5-9b-chat</th>\n",
       "      <td>4.37</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-1.1-2b-it</th>\n",
       "      <td>2.91</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-7b-chat</th>\n",
       "      <td>2.58</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-7b-it</th>\n",
       "      <td>1.82</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-70b-chat</th>\n",
       "      <td>0.81</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-0.5b-chat</th>\n",
       "      <td>0.12</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-1.8b-chat</th>\n",
       "      <td>0.00</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                -, clemscore ow/gt\n",
       "models                                            \n",
       "gpt-4-turbo-2024-04-09                 58.30    $$\n",
       "gpt-4-0125-preview                     52.50    $$\n",
       "gpt-4-1106-preview                     51.99    $$\n",
       "gpt-4-0613                             51.09    $$\n",
       "gpt-4o-2024-05-13                      48.34    $$\n",
       "claude-3-opus-20240229                 42.42    $$\n",
       "gemini-1.5-pro-latest                  41.72    $$\n",
       "llama-3-70b-instruct                   35.11    ow\n",
       "claude-2.1                             32.50    $$\n",
       "gemini-1.5-flash-latest                32.00    $$\n",
       "claude-3-sonnet-20240229               30.53    $$\n",
       "qwen1.5-72b-chat                       30.37    ow\n",
       "mistral-large-2402                     28.17    $$\n",
       "gpt-3.5-turbo-0125                     27.22    $$\n",
       "gemini-1.0-pro                         26.95    $$\n",
       "command-r-plus                         24.94    ow\n",
       "openchat_3.5                           23.64    ow\n",
       "claude-3-haiku-20240307                22.49    $$\n",
       "sheep-duck-llama-2-70b-v1.1            21.50    ow\n",
       "llama-3-8b-instruct                    19.99    ow\n",
       "openchat-3.5-1210                      18.22    ow\n",
       "wizardlm-70b-v1.0                      17.40    ow\n",
       "openchat-3.5-0106                      17.10    ow\n",
       "qwen1.5-14b-chat                       16.80    ow\n",
       "mistral-medium-2312                    16.43    $$\n",
       "qwen1.5-32b-chat                       15.41    ow\n",
       "codegemma-7b-it                        15.30    ow\n",
       "dolphin-2.5-mixtral-8x7b               15.10    ow\n",
       "codellama-34b-instruct                 14.35    ow\n",
       "command-r                              14.15    ow\n",
       "gemma-1.1-7b-it                        14.14    ow\n",
       "sus-chat-34b                           14.11    ow\n",
       "mixtral-8x22b-instruct-v0.1            12.69    ow\n",
       "tulu-2-dpo-70b                         12.62    ow\n",
       "nous-hermes-2-mixtral-8x7b-sft         11.95    ow\n",
       "wizardlm-13b-v1.2                      11.48    ow\n",
       "vicuna-33b-v1.3                        11.27    ow\n",
       "mistral-7b-instruct-v0.2                9.75    ow\n",
       "yi-34b-chat                             8.27    ow\n",
       "mixtral-8x7b-instruct-v0.1              8.17    ow\n",
       "mistral-7b-instruct-v0.1                8.01    ow\n",
       "yi-1.5-34b-chat                         7.67    ow\n",
       "vicuna-13b-v1.5                         7.01    ow\n",
       "yi-1.5-6b-chat                          6.73    ow\n",
       "starling-lm-7b-beta                     6.56    ow\n",
       "sheep-duck-llama-2-13b                  5.39    ow\n",
       "yi-1.5-9b-chat                          4.37    ow\n",
       "gemma-1.1-2b-it                         2.91    ow\n",
       "qwen1.5-7b-chat                         2.58    ow\n",
       "gemma-7b-it                             1.82    ow\n",
       "llama-2-70b-chat                        0.81    ow\n",
       "qwen1.5-0.5b-chat                       0.12    ow\n",
       "qwen1.5-1.8b-chat                       0.00    ow"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.sort_values(by=colname_score, ascending=False)[[colname_score, 'ow/gt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d76904",
   "metadata": {},
   "source": [
    "Distance best open (llama-3-70b-instruct) to best closed (gpt-4-turbo-2024-04-09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de189ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.939999999999998"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "58.30 - 33.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b479ce6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/3079169957.py:2: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  v2.loc[v2[colname_score] > 16].sort_values(by=colname_score,\n"
     ]
    }
   ],
   "source": [
    "### capped at 16, to save space\n",
    "v2.loc[v2[colname_score] > 16].sort_values(by=colname_score, \n",
    "               ascending=False)[[colname_score, 'ow/gt']].to_latex(\n",
    "    'Output/v1.6-score-table.tex',\n",
    "    header=['sc', 'o/g'],\n",
    "    float_format=\"%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced43be",
   "metadata": {},
   "source": [
    "Note: The LaTeX needs to be edited manually, as I can't seem to get it to stop putting the index label on its own header row.\n",
    "\n",
    "Needs to look like this:\n",
    "```\n",
    "\\begin{tabular}[t]{lrl}\n",
    "\\toprule\n",
    "models &    sc & o/g \\\\\n",
    "\\midrule\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaacd62",
   "metadata": {},
   "source": [
    "##### v1 = v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b61a529a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codellama-34b-instruct-hf',\n",
       " 'mistral-7b-instruct-v0.1',\n",
       " 'mixtral-8x7b-instruct-v0.1',\n",
       " 'nous-hermes-2-mixtral-8x7b-dpo',\n",
       " 'sus-chat-34b',\n",
       " 'wizard-vicuna-13b-uncensored-hf',\n",
       " 'wizardlm-13b-v1.2',\n",
       " 'wizardlm-70b-v1.0',\n",
       " 'yi-34b-chat',\n",
       " 'claude-2',\n",
       " 'claude-2.1',\n",
       " 'claude-instant-1.2',\n",
       " 'claude-v1.3',\n",
       " 'command',\n",
       " 'deepseek-llm-67b-chat',\n",
       " 'deepseek-llm-7b-chat',\n",
       " 'falcon-7b-instruct',\n",
       " 'gpt-3.5-turbo-0613',\n",
       " 'gpt-3.5-turbo-1106',\n",
       " 'gpt-4-0314',\n",
       " 'gpt-4-0613',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt4all-13b-snoozy',\n",
       " 'koala-13b-hf',\n",
       " 'llama-2-13b-chat-hf',\n",
       " 'llama-2-70b-chat-hf',\n",
       " 'llama-2-7b-chat-hf',\n",
       " 'mistral-medium',\n",
       " 'oasst-sft-4-pythia-12b-epoch-3.5',\n",
       " 'openchat-3.5-0106',\n",
       " 'openchat-3.5-1210',\n",
       " 'openchat_3.5',\n",
       " 'sheep-duck-llama-2-13b',\n",
       " 'sheep-duck-llama-2-70b-v1.1',\n",
       " 'tulu-2-dpo-70b',\n",
       " 'tulu-2-dpo-7b',\n",
       " 'vicuna-13b-v1.5',\n",
       " 'vicuna-33b-v1.3',\n",
       " 'vicuna-7b-v1.5',\n",
       " 'zephyr-7b-alpha',\n",
       " 'zephyr-7b-beta']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04a0451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_open = [\n",
    "    'codellama-34b-instruct-hf',\n",
    " 'mistral-7b-instruct-v0.1',\n",
    " 'mixtral-8x7b-instruct-v0.1',\n",
    " 'nous-hermes-2-mixtral-8x7b-dpo',\n",
    " 'sus-chat-34b',\n",
    " 'wizard-vicuna-13b-uncensored-hf',\n",
    " 'wizardlm-13b-v1.2',\n",
    " 'wizardlm-70b-v1.0',\n",
    " 'yi-34b-chat',\n",
    " 'deepseek-llm-67b-chat',\n",
    " 'deepseek-llm-7b-chat',\n",
    " 'falcon-7b-instruct',\n",
    " 'koala-13b-hf',\n",
    " 'llama-2-13b-chat-hf',\n",
    " 'llama-2-70b-chat-hf',\n",
    " 'llama-2-7b-chat-hf',\n",
    " 'mistral-medium',\n",
    " 'oasst-sft-4-pythia-12b-epoch-3.5',\n",
    " 'openchat-3.5-0106',\n",
    " 'openchat-3.5-1210',\n",
    " 'openchat_3.5',\n",
    " 'sheep-duck-llama-2-13b',\n",
    " 'sheep-duck-llama-2-70b-v1.1',\n",
    " 'tulu-2-dpo-70b',\n",
    " 'tulu-2-dpo-7b',\n",
    " 'vicuna-13b-v1.5',\n",
    " 'vicuna-33b-v1.3',\n",
    " 'vicuna-7b-v1.5',\n",
    " 'zephyr-7b-alpha',\n",
    " 'zephyr-7b-beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91402187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1.loc[:, 'ow/gt'] = '🔒'\n",
    "#v1.loc[v1.index.isin(v1_open), 'ow/gt'] = '🏋'\n",
    "\n",
    "v1.loc[:, 'ow/gt'] = '$$'\n",
    "v1.loc[v1.index.isin(v1_open), 'ow/gt'] = 'ow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21c776a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-, clemscore</th>\n",
       "      <th>ow/gt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4-0613</th>\n",
       "      <td>60.90</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-1106-preview</th>\n",
       "      <td>60.33</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0314</th>\n",
       "      <td>58.81</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-v1.3</th>\n",
       "      <td>37.64</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-2.1</th>\n",
       "      <td>36.38</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-2</th>\n",
       "      <td>33.71</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0613</th>\n",
       "      <td>32.53</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-1106</th>\n",
       "      <td>30.45</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat_3.5</th>\n",
       "      <td>19.72</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-medium</th>\n",
       "      <td>17.99</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixtral-8x7b-instruct-v0.1</th>\n",
       "      <td>17.81</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat-3.5-1210</th>\n",
       "      <td>17.61</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sheep-duck-llama-2-70b-v1.1</th>\n",
       "      <td>17.12</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-34b-chat</th>\n",
       "      <td>16.77</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-70b-v1.0</th>\n",
       "      <td>16.70</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tulu-2-dpo-70b</th>\n",
       "      <td>15.90</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sus-chat-34b</th>\n",
       "      <td>15.64</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-instant-1.2</th>\n",
       "      <td>15.44</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat-3.5-0106</th>\n",
       "      <td>14.33</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nous-hermes-2-mixtral-8x7b-dpo</th>\n",
       "      <td>12.69</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codellama-34b-instruct-hf</th>\n",
       "      <td>10.34</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-33b-v1.3</th>\n",
       "      <td>9.15</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-13b-v1.2</th>\n",
       "      <td>7.82</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.5</th>\n",
       "      <td>7.21</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sheep-duck-llama-2-13b</th>\n",
       "      <td>6.74</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-7b-v1.5</th>\n",
       "      <td>3.46</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tulu-2-dpo-7b</th>\n",
       "      <td>3.27</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>command</th>\n",
       "      <td>3.12</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizard-vicuna-13b-uncensored-hf</th>\n",
       "      <td>2.06</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-13b-chat-hf</th>\n",
       "      <td>1.89</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-instruct-v0.1</th>\n",
       "      <td>1.50</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-70b-chat-hf</th>\n",
       "      <td>1.39</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koala-13b-hf</th>\n",
       "      <td>1.25</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-7b-beta</th>\n",
       "      <td>1.23</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek-llm-67b-chat</th>\n",
       "      <td>0.77</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-7b-alpha</th>\n",
       "      <td>0.75</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-7b-chat-hf</th>\n",
       "      <td>0.24</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4all-13b-snoozy</th>\n",
       "      <td>0.00</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek-llm-7b-chat</th>\n",
       "      <td>0.00</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oasst-sft-4-pythia-12b-epoch-3.5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-instruct</th>\n",
       "      <td>0.00</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  -, clemscore ow/gt\n",
       "models                                              \n",
       "gpt-4-0613                               60.90    $$\n",
       "gpt-4-1106-preview                       60.33    $$\n",
       "gpt-4-0314                               58.81    $$\n",
       "claude-v1.3                              37.64    $$\n",
       "claude-2.1                               36.38    $$\n",
       "claude-2                                 33.71    $$\n",
       "gpt-3.5-turbo-0613                       32.53    $$\n",
       "gpt-3.5-turbo-1106                       30.45    $$\n",
       "openchat_3.5                             19.72    ow\n",
       "mistral-medium                           17.99    ow\n",
       "mixtral-8x7b-instruct-v0.1               17.81    ow\n",
       "openchat-3.5-1210                        17.61    ow\n",
       "sheep-duck-llama-2-70b-v1.1              17.12    ow\n",
       "yi-34b-chat                              16.77    ow\n",
       "wizardlm-70b-v1.0                        16.70    ow\n",
       "tulu-2-dpo-70b                           15.90    ow\n",
       "sus-chat-34b                             15.64    ow\n",
       "claude-instant-1.2                       15.44    $$\n",
       "openchat-3.5-0106                        14.33    ow\n",
       "nous-hermes-2-mixtral-8x7b-dpo           12.69    ow\n",
       "codellama-34b-instruct-hf                10.34    ow\n",
       "vicuna-33b-v1.3                           9.15    ow\n",
       "wizardlm-13b-v1.2                         7.82    ow\n",
       "vicuna-13b-v1.5                           7.21    ow\n",
       "sheep-duck-llama-2-13b                    6.74    ow\n",
       "vicuna-7b-v1.5                            3.46    ow\n",
       "tulu-2-dpo-7b                             3.27    ow\n",
       "command                                   3.12    $$\n",
       "wizard-vicuna-13b-uncensored-hf           2.06    ow\n",
       "llama-2-13b-chat-hf                       1.89    ow\n",
       "mistral-7b-instruct-v0.1                  1.50    ow\n",
       "llama-2-70b-chat-hf                       1.39    ow\n",
       "koala-13b-hf                              1.25    ow\n",
       "zephyr-7b-beta                            1.23    ow\n",
       "deepseek-llm-67b-chat                     0.77    ow\n",
       "zephyr-7b-alpha                           0.75    ow\n",
       "llama-2-7b-chat-hf                        0.24    ow\n",
       "gpt4all-13b-snoozy                        0.00    $$\n",
       "deepseek-llm-7b-chat                      0.00    ow\n",
       "oasst-sft-4-pythia-12b-epoch-3.5          0.00    ow\n",
       "falcon-7b-instruct                        0.00    ow"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.sort_values(by=colname_score, ascending=False)[[colname_score, 'ow/gt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b1356c",
   "metadata": {},
   "source": [
    "Distance best open (openchat_3.5) to best closed (gpt-4-0613\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c722d639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.18"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60.90 - 19.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577f34b",
   "metadata": {},
   "source": [
    "Do I even need to cap this? .... I leave it at 15 for now, even though v1.6 is capped at 15..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "759e3077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/3526560759.py:2: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  v1.loc[v1[colname_score] > 15].sort_values(by=colname_score,\n"
     ]
    }
   ],
   "source": [
    "### capped at 15, to save space\n",
    "v1.loc[v1[colname_score] > 15].sort_values(by=colname_score, \n",
    "               ascending=False)[[colname_score, 'ow/gt']].to_latex(\n",
    "    'Output/v1.0-score-table.tex',\n",
    "    header=['sc', 'o/g'],\n",
    "    float_format=\"%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9438c21b",
   "metadata": {},
   "source": [
    "##### v0 = v0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "341e21b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['claude-v1.3',\n",
       " 'falcon-40b',\n",
       " 'gpt-3.5-turbo',\n",
       " 'gpt-4',\n",
       " 'koala-13b',\n",
       " 'luminous-supreme',\n",
       " 'oasst-12b',\n",
       " 'text-davinci-003',\n",
       " 'vicuna-13b']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86f42f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v0_closed = [\n",
    "    'claude-v1.3',\n",
    " 'gpt-3.5-turbo',\n",
    " 'gpt-4',\n",
    " 'luminous-supreme',\n",
    " 'text-davinci-003',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc2ee436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.loc[:, 'ow/gt'] = '🏋🏽‍♀️'\n",
    "#v0.loc[v0.index.isin(v0_closed), 'ow/gt'] = '🔒'\n",
    "\n",
    "v0.loc[:, 'ow/gt'] = 'ow'\n",
    "v0.loc[v0.index.isin(v0_closed), 'ow/gt'] = '$$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa5b1214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-, clemscore</th>\n",
       "      <th>ow/gt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>59.49</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-v1.3</th>\n",
       "      <td>37.07</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>37.02</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-003</th>\n",
       "      <td>15.78</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b</th>\n",
       "      <td>4.24</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oasst-12b</th>\n",
       "      <td>1.74</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koala-13b</th>\n",
       "      <td>1.48</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-40b</th>\n",
       "      <td>0.71</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luminous-supreme</th>\n",
       "      <td>0.00</td>\n",
       "      <td>$$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  -, clemscore ow/gt\n",
       "models                              \n",
       "gpt-4                    59.49    $$\n",
       "claude-v1.3              37.07    $$\n",
       "gpt-3.5-turbo            37.02    $$\n",
       "text-davinci-003         15.78    $$\n",
       "vicuna-13b                4.24    ow\n",
       "oasst-12b                 1.74    ow\n",
       "koala-13b                 1.48    ow\n",
       "falcon-40b                0.71    ow\n",
       "luminous-supreme          0.00    $$"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0.sort_values(by=colname_score, ascending=False)[[colname_score, 'ow/gt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835c952",
   "metadata": {},
   "source": [
    "Distance best open (vicuna-13b) to best closed (gpt-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5eeb82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.25"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59.49 - 4.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b334573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/2082067986.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  v0.sort_values(by=colname_score,\n"
     ]
    }
   ],
   "source": [
    "v0.sort_values(by=colname_score, \n",
    "               ascending=False)[[colname_score, 'ow/gt']].to_latex(\n",
    "    'Output/v0.9-score-table.tex',\n",
    "    header=['sc', 'o/g'],\n",
    "    float_format=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a839f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6062d1e",
   "metadata": {},
   "source": [
    "#### for the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee7551d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/328806768.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  v2.iloc[:,[0,1,2,-1]].sort_values(by=colname_score, ascending=False).to_latex(\n",
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/328806768.py:6: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  v1.iloc[:,[0,1,2,-1]].sort_values(by=colname_score, ascending=False).to_latex(\n",
      "/var/folders/v4/sxby0fb08xjbdx001s6j3wrh0000gq/T/ipykernel_92468/328806768.py:11: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  v0.iloc[:,[0,1,2,-1]].sort_values(by=colname_score, ascending=False).to_latex(\n"
     ]
    }
   ],
   "source": [
    "v2.iloc[:,[0,1,2,-1]].sort_values(by=colname_score, ascending=False).to_latex(\n",
    "    'Output/v1.6-full-table.tex',\n",
    "    header=['sc', '%pl', 'qs', 'o/g'],\n",
    "    float_format=\"%.2f\"\n",
    "    )\n",
    "v1.iloc[:,[0,1,2,-1]].sort_values(by=colname_score, ascending=False).to_latex(\n",
    "    'Output/v1.0-full-table.tex',\n",
    "    header=['sc', '%pl', 'qs', 'o/g'],\n",
    "    float_format=\"%.2f\"\n",
    "    )\n",
    "v0.iloc[:,[0,1,2,-1]].sort_values(by=colname_score, ascending=False).to_latex(\n",
    "    'Output/v0.9-full-table.tex',\n",
    "    header=['sc', '%pl', 'qs', 'o/g'],\n",
    "    float_format=\"%.2f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa269cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8705f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a4bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28668d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7022d6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-, clemscore</th>\n",
       "      <th>all, Average % Played</th>\n",
       "      <th>all, Average Quality Score</th>\n",
       "      <th>imagegame, % Played</th>\n",
       "      <th>imagegame, Quality Score</th>\n",
       "      <th>imagegame, Quality Score (std)</th>\n",
       "      <th>privateshared, % Played</th>\n",
       "      <th>privateshared, Quality Score</th>\n",
       "      <th>privateshared, Quality Score (std)</th>\n",
       "      <th>referencegame, % Played</th>\n",
       "      <th>referencegame, Quality Score</th>\n",
       "      <th>referencegame, Quality Score (std)</th>\n",
       "      <th>taboo, % Played</th>\n",
       "      <th>taboo, Quality Score</th>\n",
       "      <th>taboo, Quality Score (std)</th>\n",
       "      <th>wordle, % Played</th>\n",
       "      <th>wordle, Quality Score</th>\n",
       "      <th>wordle, Quality Score (std)</th>\n",
       "      <th>wordle_withclue, % Played</th>\n",
       "      <th>wordle_withclue, Quality Score</th>\n",
       "      <th>wordle_withclue, Quality Score (std)</th>\n",
       "      <th>wordle_withcritic, % Played</th>\n",
       "      <th>wordle_withcritic, Quality Score</th>\n",
       "      <th>wordle_withcritic, Quality Score (std)</th>\n",
       "      <th>ow/gt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codellama-34b-instruct</th>\n",
       "      <td>14.35</td>\n",
       "      <td>33.57</td>\n",
       "      <td>42.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>94.44</td>\n",
       "      <td>22.97</td>\n",
       "      <td>51.67</td>\n",
       "      <td>51.61</td>\n",
       "      <td>50.80</td>\n",
       "      <td>56.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.67</td>\n",
       "      <td>25.00</td>\n",
       "      <td>46.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-3-70b-instruct</th>\n",
       "      <td>35.11</td>\n",
       "      <td>80.72</td>\n",
       "      <td>43.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>84.37</td>\n",
       "      <td>13.69</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.44</td>\n",
       "      <td>48.00</td>\n",
       "      <td>91.67</td>\n",
       "      <td>70.30</td>\n",
       "      <td>39.37</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.81</td>\n",
       "      <td>96.67</td>\n",
       "      <td>14.37</td>\n",
       "      <td>32.34</td>\n",
       "      <td>86.67</td>\n",
       "      <td>25.64</td>\n",
       "      <td>39.08</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-3-8b-instruct</th>\n",
       "      <td>19.99</td>\n",
       "      <td>76.10</td>\n",
       "      <td>26.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "      <td>58.91</td>\n",
       "      <td>30.05</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.11</td>\n",
       "      <td>49.99</td>\n",
       "      <td>100.00</td>\n",
       "      <td>37.78</td>\n",
       "      <td>45.08</td>\n",
       "      <td>86.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>83.33</td>\n",
       "      <td>14.00</td>\n",
       "      <td>33.91</td>\n",
       "      <td>66.67</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.73</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-instruct-v0.1</th>\n",
       "      <td>8.01</td>\n",
       "      <td>37.14</td>\n",
       "      <td>21.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.58</td>\n",
       "      <td>100.0</td>\n",
       "      <td>55.00</td>\n",
       "      <td>49.89</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.67</td>\n",
       "      <td>45.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.67</td>\n",
       "      <td>20.00</td>\n",
       "      <td>44.72</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-instruct-v0.2</th>\n",
       "      <td>9.75</td>\n",
       "      <td>36.91</td>\n",
       "      <td>26.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.33</td>\n",
       "      <td>48.76</td>\n",
       "      <td>65.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.67</td>\n",
       "      <td>43.75</td>\n",
       "      <td>49.55</td>\n",
       "      <td>16.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          -, clemscore  all, Average % Played  \\\n",
       "models                                                          \n",
       "codellama-34b-instruct           14.35                  33.57   \n",
       "llama-3-70b-instruct             35.11                  80.72   \n",
       "llama-3-8b-instruct              19.99                  76.10   \n",
       "mistral-7b-instruct-v0.1          8.01                  37.14   \n",
       "mistral-7b-instruct-v0.2          9.75                  36.91   \n",
       "\n",
       "                          all, Average Quality Score  imagegame, % Played  \\\n",
       "models                                                                      \n",
       "codellama-34b-instruct                         42.76                  0.0   \n",
       "llama-3-70b-instruct                           43.50                  0.0   \n",
       "llama-3-8b-instruct                            26.27                  0.0   \n",
       "mistral-7b-instruct-v0.1                       21.58                  0.0   \n",
       "mistral-7b-instruct-v0.2                       26.42                  0.0   \n",
       "\n",
       "                          imagegame, Quality Score  \\\n",
       "models                                               \n",
       "codellama-34b-instruct                         NaN   \n",
       "llama-3-70b-instruct                           NaN   \n",
       "llama-3-8b-instruct                            NaN   \n",
       "mistral-7b-instruct-v0.1                       NaN   \n",
       "mistral-7b-instruct-v0.2                       NaN   \n",
       "\n",
       "                          imagegame, Quality Score (std)  \\\n",
       "models                                                     \n",
       "codellama-34b-instruct                               NaN   \n",
       "llama-3-70b-instruct                                 NaN   \n",
       "llama-3-8b-instruct                                  NaN   \n",
       "mistral-7b-instruct-v0.1                             NaN   \n",
       "mistral-7b-instruct-v0.2                             NaN   \n",
       "\n",
       "                          privateshared, % Played  \\\n",
       "models                                              \n",
       "codellama-34b-instruct                        0.0   \n",
       "llama-3-70b-instruct                        100.0   \n",
       "llama-3-8b-instruct                          96.0   \n",
       "mistral-7b-instruct-v0.1                     20.0   \n",
       "mistral-7b-instruct-v0.2                      0.0   \n",
       "\n",
       "                          privateshared, Quality Score  \\\n",
       "models                                                   \n",
       "codellama-34b-instruct                             NaN   \n",
       "llama-3-70b-instruct                             84.37   \n",
       "llama-3-8b-instruct                              58.91   \n",
       "mistral-7b-instruct-v0.1                          1.21   \n",
       "mistral-7b-instruct-v0.2                           NaN   \n",
       "\n",
       "                          privateshared, Quality Score (std)  \\\n",
       "models                                                         \n",
       "codellama-34b-instruct                                   NaN   \n",
       "llama-3-70b-instruct                                   13.69   \n",
       "llama-3-8b-instruct                                    30.05   \n",
       "mistral-7b-instruct-v0.1                                2.58   \n",
       "mistral-7b-instruct-v0.2                                 NaN   \n",
       "\n",
       "                          referencegame, % Played  \\\n",
       "models                                              \n",
       "codellama-34b-instruct                      100.0   \n",
       "llama-3-70b-instruct                        100.0   \n",
       "llama-3-8b-instruct                         100.0   \n",
       "mistral-7b-instruct-v0.1                    100.0   \n",
       "mistral-7b-instruct-v0.2                    100.0   \n",
       "\n",
       "                          referencegame, Quality Score  \\\n",
       "models                                                   \n",
       "codellama-34b-instruct                           94.44   \n",
       "llama-3-70b-instruct                             64.44   \n",
       "llama-3-8b-instruct                              46.11   \n",
       "mistral-7b-instruct-v0.1                         55.00   \n",
       "mistral-7b-instruct-v0.2                         38.33   \n",
       "\n",
       "                          referencegame, Quality Score (std)  taboo, % Played  \\\n",
       "models                                                                          \n",
       "codellama-34b-instruct                                 22.97            51.67   \n",
       "llama-3-70b-instruct                                   48.00            91.67   \n",
       "llama-3-8b-instruct                                    49.99           100.00   \n",
       "mistral-7b-instruct-v0.1                               49.89           100.00   \n",
       "mistral-7b-instruct-v0.2                               48.76            65.00   \n",
       "\n",
       "                          taboo, Quality Score  taboo, Quality Score (std)  \\\n",
       "models                                                                       \n",
       "codellama-34b-instruct                   51.61                       50.80   \n",
       "llama-3-70b-instruct                     70.30                       39.37   \n",
       "llama-3-8b-instruct                      37.78                       45.08   \n",
       "mistral-7b-instruct-v0.1                 31.67                       45.07   \n",
       "mistral-7b-instruct-v0.2                  0.00                        0.00   \n",
       "\n",
       "                          wordle, % Played  wordle, Quality Score  \\\n",
       "models                                                              \n",
       "codellama-34b-instruct               56.67                   0.00   \n",
       "llama-3-70b-instruct                 90.00                   1.85   \n",
       "llama-3-8b-instruct                  86.67                   0.00   \n",
       "mistral-7b-instruct-v0.1              0.00                    NaN   \n",
       "mistral-7b-instruct-v0.2             50.00                   0.00   \n",
       "\n",
       "                          wordle, Quality Score (std)  \\\n",
       "models                                                  \n",
       "codellama-34b-instruct                           0.00   \n",
       "llama-3-70b-instruct                             6.81   \n",
       "llama-3-8b-instruct                              0.00   \n",
       "mistral-7b-instruct-v0.1                          NaN   \n",
       "mistral-7b-instruct-v0.2                         0.00   \n",
       "\n",
       "                          wordle_withclue, % Played  \\\n",
       "models                                                \n",
       "codellama-34b-instruct                        26.67   \n",
       "llama-3-70b-instruct                          96.67   \n",
       "llama-3-8b-instruct                           83.33   \n",
       "mistral-7b-instruct-v0.1                      23.33   \n",
       "mistral-7b-instruct-v0.2                      26.67   \n",
       "\n",
       "                          wordle_withclue, Quality Score  \\\n",
       "models                                                     \n",
       "codellama-34b-instruct                             25.00   \n",
       "llama-3-70b-instruct                               14.37   \n",
       "llama-3-8b-instruct                                14.00   \n",
       "mistral-7b-instruct-v0.1                            0.00   \n",
       "mistral-7b-instruct-v0.2                           43.75   \n",
       "\n",
       "                          wordle_withclue, Quality Score (std)  \\\n",
       "models                                                           \n",
       "codellama-34b-instruct                                   46.29   \n",
       "llama-3-70b-instruct                                     32.34   \n",
       "llama-3-8b-instruct                                      33.91   \n",
       "mistral-7b-instruct-v0.1                                  0.00   \n",
       "mistral-7b-instruct-v0.2                                 49.55   \n",
       "\n",
       "                          wordle_withcritic, % Played  \\\n",
       "models                                                  \n",
       "codellama-34b-instruct                           0.00   \n",
       "llama-3-70b-instruct                            86.67   \n",
       "llama-3-8b-instruct                             66.67   \n",
       "mistral-7b-instruct-v0.1                        16.67   \n",
       "mistral-7b-instruct-v0.2                        16.67   \n",
       "\n",
       "                          wordle_withcritic, Quality Score  \\\n",
       "models                                                       \n",
       "codellama-34b-instruct                                 NaN   \n",
       "llama-3-70b-instruct                                 25.64   \n",
       "llama-3-8b-instruct                                   0.83   \n",
       "mistral-7b-instruct-v0.1                             20.00   \n",
       "mistral-7b-instruct-v0.2                             50.00   \n",
       "\n",
       "                          wordle_withcritic, Quality Score (std) ow/gt  \n",
       "models                                                                  \n",
       "codellama-34b-instruct                                       NaN    ow  \n",
       "llama-3-70b-instruct                                       39.08    ow  \n",
       "llama-3-8b-instruct                                         3.73    ow  \n",
       "mistral-7b-instruct-v0.1                                   44.72    ow  \n",
       "mistral-7b-instruct-v0.2                                   50.00    ow  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "954625c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='all, Average % Played', ylabel='all, Average Quality Score'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v2.plot.scatter(colname_played, colname_quality, xlim=(0,100), ylim=(0,100), figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2da8813",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-, clemscore</th>\n",
       "      <th>all, Average % Played</th>\n",
       "      <th>all, Average Quality Score</th>\n",
       "      <th>imagegame, % Played</th>\n",
       "      <th>imagegame, Quality Score</th>\n",
       "      <th>imagegame, Quality Score (std)</th>\n",
       "      <th>privateshared, % Played</th>\n",
       "      <th>privateshared, Quality Score</th>\n",
       "      <th>privateshared, Quality Score (std)</th>\n",
       "      <th>referencegame, % Played</th>\n",
       "      <th>referencegame, Quality Score</th>\n",
       "      <th>referencegame, Quality Score (std)</th>\n",
       "      <th>taboo, % Played</th>\n",
       "      <th>taboo, Quality Score</th>\n",
       "      <th>taboo, Quality Score (std)</th>\n",
       "      <th>wordle, % Played</th>\n",
       "      <th>wordle, Quality Score</th>\n",
       "      <th>wordle, Quality Score (std)</th>\n",
       "      <th>wordle_withclue, % Played</th>\n",
       "      <th>wordle_withclue, Quality Score</th>\n",
       "      <th>wordle_withclue, Quality Score (std)</th>\n",
       "      <th>wordle_withcritic, % Played</th>\n",
       "      <th>wordle_withcritic, Quality Score</th>\n",
       "      <th>wordle_withcritic, Quality Score (std)</th>\n",
       "      <th>ow/gt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>qwen1.5-0.5b-chat</th>\n",
       "      <td>0.12</td>\n",
       "      <td>25.72</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.67</td>\n",
       "      <td>1.92</td>\n",
       "      <td>13.87</td>\n",
       "      <td>46.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-1.8b-chat</th>\n",
       "      <td>0.00</td>\n",
       "      <td>15.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1.5-7b-chat</th>\n",
       "      <td>2.58</td>\n",
       "      <td>30.24</td>\n",
       "      <td>8.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.56</td>\n",
       "      <td>40.52</td>\n",
       "      <td>98.33</td>\n",
       "      <td>13.56</td>\n",
       "      <td>33.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   -, clemscore  all, Average % Played  \\\n",
       "models                                                   \n",
       "qwen1.5-0.5b-chat          0.12                  25.72   \n",
       "qwen1.5-1.8b-chat          0.00                  15.24   \n",
       "qwen1.5-7b-chat            2.58                  30.24   \n",
       "\n",
       "                   all, Average Quality Score  imagegame, % Played  \\\n",
       "models                                                               \n",
       "qwen1.5-0.5b-chat                        0.48                  0.0   \n",
       "qwen1.5-1.8b-chat                        0.00                  0.0   \n",
       "qwen1.5-7b-chat                          8.53                  0.0   \n",
       "\n",
       "                   imagegame, Quality Score  imagegame, Quality Score (std)  \\\n",
       "models                                                                        \n",
       "qwen1.5-0.5b-chat                       NaN                             NaN   \n",
       "qwen1.5-1.8b-chat                       NaN                             NaN   \n",
       "qwen1.5-7b-chat                         NaN                             NaN   \n",
       "\n",
       "                   privateshared, % Played  privateshared, Quality Score  \\\n",
       "models                                                                     \n",
       "qwen1.5-0.5b-chat                      0.0                           NaN   \n",
       "qwen1.5-1.8b-chat                      0.0                           NaN   \n",
       "qwen1.5-7b-chat                        0.0                           NaN   \n",
       "\n",
       "                   privateshared, Quality Score (std)  \\\n",
       "models                                                  \n",
       "qwen1.5-0.5b-chat                                 NaN   \n",
       "qwen1.5-1.8b-chat                                 NaN   \n",
       "qwen1.5-7b-chat                                   NaN   \n",
       "\n",
       "                   referencegame, % Played  referencegame, Quality Score  \\\n",
       "models                                                                     \n",
       "qwen1.5-0.5b-chat                      0.0                           NaN   \n",
       "qwen1.5-1.8b-chat                      0.0                           NaN   \n",
       "qwen1.5-7b-chat                      100.0                         20.56   \n",
       "\n",
       "                   referencegame, Quality Score (std)  taboo, % Played  \\\n",
       "models                                                                   \n",
       "qwen1.5-0.5b-chat                                 NaN            86.67   \n",
       "qwen1.5-1.8b-chat                                 NaN            93.33   \n",
       "qwen1.5-7b-chat                                 40.52            98.33   \n",
       "\n",
       "                   taboo, Quality Score  taboo, Quality Score (std)  \\\n",
       "models                                                                \n",
       "qwen1.5-0.5b-chat                  1.92                       13.87   \n",
       "qwen1.5-1.8b-chat                  0.00                        0.00   \n",
       "qwen1.5-7b-chat                   13.56                       33.26   \n",
       "\n",
       "                   wordle, % Played  wordle, Quality Score  \\\n",
       "models                                                       \n",
       "qwen1.5-0.5b-chat             46.67                    0.0   \n",
       "qwen1.5-1.8b-chat              0.00                    NaN   \n",
       "qwen1.5-7b-chat                0.00                    NaN   \n",
       "\n",
       "                   wordle, Quality Score (std)  wordle_withclue, % Played  \\\n",
       "models                                                                      \n",
       "qwen1.5-0.5b-chat                          0.0                       40.0   \n",
       "qwen1.5-1.8b-chat                          NaN                       10.0   \n",
       "qwen1.5-7b-chat                            NaN                       10.0   \n",
       "\n",
       "                   wordle_withclue, Quality Score  \\\n",
       "models                                              \n",
       "qwen1.5-0.5b-chat                             0.0   \n",
       "qwen1.5-1.8b-chat                             0.0   \n",
       "qwen1.5-7b-chat                               0.0   \n",
       "\n",
       "                   wordle_withclue, Quality Score (std)  \\\n",
       "models                                                    \n",
       "qwen1.5-0.5b-chat                                   0.0   \n",
       "qwen1.5-1.8b-chat                                   0.0   \n",
       "qwen1.5-7b-chat                                     0.0   \n",
       "\n",
       "                   wordle_withcritic, % Played  \\\n",
       "models                                           \n",
       "qwen1.5-0.5b-chat                         6.67   \n",
       "qwen1.5-1.8b-chat                         3.33   \n",
       "qwen1.5-7b-chat                           3.33   \n",
       "\n",
       "                   wordle_withcritic, Quality Score  \\\n",
       "models                                                \n",
       "qwen1.5-0.5b-chat                               0.0   \n",
       "qwen1.5-1.8b-chat                               0.0   \n",
       "qwen1.5-7b-chat                                 0.0   \n",
       "\n",
       "                   wordle_withcritic, Quality Score (std) ow/gt  \n",
       "models                                                           \n",
       "qwen1.5-0.5b-chat                                     0.0    ow  \n",
       "qwen1.5-1.8b-chat                                     NaN    ow  \n",
       "qwen1.5-7b-chat                                       NaN    ow  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.loc[v2[colname_quality] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d0561d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all, Average % Played'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colname_played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdf033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "ctrl-q"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
