# Benchmark Runs

## **v0.9** - June 2023

Evaluated Models:
- GPT-4  
- GPT-3.5-turbo
- Text-davinci-003
- Claude v1.3
- Luminous-supreme
- Falcon-40B
- Koala-13B
- Open-assistant-12B
- Vicuna-13B


### Leaderboard of the run is available here: [LINK](url)

### Transcripts of the dialog games 

All generated transcripts by each model for each game can be found under [transcripts folder](url). The outputs are organised as follows: `game/model/experiment`.

### Benchmark files

The files generated by the evaluation script for all models can be found under [evaluation folder](). 

The [`evaluation/results_eval`](url) folder includes plots and tables averages across all games and their experiments.

There are also plots and tables for each game individually, which are averaged across all experiments for that specific game. These can be found under `evaluation\{GAME_NAME}`.

